{
	"auto_complete":
	{
		"selected_items":
		[
		]
	},
	"buffers":
	[
		{
			"contents": "% This is samplepaper.tex, a sample chapter demonstrating the\n% LLNCS macro package for Springer Computer Science proceedings;\n% Version 2.20 of 2017/10/04\n%\n\\documentclass[runningheads]{llncs}\n%\n\\usepackage{graphicx}\n% Used for displaying a sample figure. If possible, figure files should\n% be included in EPS format.\n%\n% If you use the hyperref package, please uncomment the following line\n% to display URLs in blue roman font according to Springer's eBook style:\n% \\renewcommand\\UrlFont{\\color{blue}\\rmfamily}\n\n\\begin{document}\n%\n\\title{An Overview of Model-Based Testing\\thanks{Supported by organization x.}}\n%\n%\\titlerunning{Abbreviated paper title}\n% If the paper title is too long for the running head, you can set\n% an abbreviated paper title here\n%\n\\author{Jorge Villalobos Rebollosa\\inst{1}\\orcidID{0000-1111-2222-3333} \\and\nSecond Author\\inst{2,3}\\orcidID{1111-2222-3333-4444} \\and\nThird Author\\inst{3}\\orcidID{2222--3333-4444-5555}}\n%\n\n\\authorrunning{F. Author et al.}\n% First names are abbreviated in the running head.\n% If there are more than two authors, 'et al.' is used.\n%\n\\institute{University of Mannheim, Mannheim, Germany }\n%\n\\maketitle              % typeset the header of the contribution\n%\n\\begin{abstract}\nThe purpose of the following document is to provide an overview of  \nModel-Based Testing (MBT), highlighting its differences with other testing approaches and especially focusing on its application alongside agile development methodologies. \nThe document will then explain how model-based testing facilitates the automation of test case generation, and how it relates to model-driven development and constraint programming.\nThe advantages and challenges of this testing methodology will be exposed, and a summary and evaluation of tools for its implementation will be provided.\n\n\n\\keywords{Testing  \\and Modeling \\and Model-Based Testing \\and Agile Development}\n\\end{abstract}\n%\n%\n%\n\\section{Introduction}\nWith the increasing adoption of Agile development methodologies and the shortening of the development cycles it has become imperative for software testers to find  better and more efficient ways to evaluate the quality of software products. The automation of test execution has reduced significantly the time required for testing while also increasing the re-usability of the generated test suite, but still it depends on the tester to generate the right test cases that satisfy the quality requirements for the system or module. This has led to the development of approaches that seek to automate the test generation process, the most common of which is known as Model-based testing.\n\\section{Model-Based Testing}\n\n\\subsection{The testing process}\n\nThe most basic task of the software tester is to generate a set of test cases (known also as Test Suite) and then run those tests in the System Under Test (SUT) in order to evaluate the quality of the system against the informal requirements. For the tester to be able to evaluate whether a test passed successfully or not, a Test Oracle is required, which is a list, method or system that provides the correct results or behavior of a system in response to a specific set of inputs. \n\nIdeally the tester will create an exhaustive set of tests that go through every aspect of the program and verify that every possible input produces the required state. In reality though, this is impossible, since the amount of tests could be infinite. Moreover, systems are developed under strict  time and budget deadlines, so only a fractional amount of the possible test cases can be executed before release. It is therefore the job of the tester to determine which set of test cases should be executed, and to provide information on which parts of the overall system have been evaluated, and which parts haven’t. \n\nThe most resource consuming part of the testing process is the execution of the previously generated tests. This is also a boring and repetitive task, and can also be very expensive since it requires a lot of human effort. It is also prone to human errors. To tackle this several test automation tools exist in the market. The goal of test automation is for the tester to use a programming language to define the test once, and then execute these test automatically whenever necessary. This allows for concepts such as Continuous Integration to be possible, since developers can now automatically trigger the regression test suite every time they push a new version of the module they are working on.\n\nHowever, the test automation of the test execution still depends on the tester to design the test suite, which means there is still the need for a test oracle, and there is still the problem of generating the right tests to ensure the desired coverage. Automated test suites also have the disadvantage of not being very flexible, since small changes on the way the system works (A change in the system’s API for example) renders a lot of tests useless.\n\n\\subsection{Model-Based Testing}\n\nNew testing approaches allow us to  take automation one step further. Instead of having the tester generate the test suite by himself, the role of the tester is now to determine an abstract version of the system based on the requirements, and then allowing tools to automatically generate a set of tests from that version. This allows the tester to automatically generate a set of tests that satisfy the desired coverage. This new role also allows the tester to perform an evaluation of the informal requirements, so ambiguous or incomplete requirements can also be quickly found. As a result, the testing process can now begin before a single line of code is written by the developers, and finding errors in the requirement gathering/design phase of the software development life cycle ends up saving a lot of time and money. This is where the Model-based testing comes into play.\n\nModel-Based Testing can therefore be defined as the “automation of the design of black box tests”. It is an application of concepts derived from the more general Model Driven Development (MDD), and can take advantage of most of the same (or similar) modeling languages. \n\n\\subsection{ Modeling for testing}\nEvery system developed has as a starting point a set of informal requirements, provided by the client or one of the shareholders. The client usually has no knowledge of how a system is built, and as such the requirements are usually expressed in a non technical language, in terms of desired results or functionality. Therefore, before a system can be built it is necessary to transform this requirements into something the developers can actually use to begin building the system. Models provide a solution for this process. Software models are abstractions of the required functionalities of a system, taking advantage of visual techniques or an abstract language to represent the intended design.\nThere are several object modeling languages and techniques that can be used to create these models, one of the most common being the Universal Modeling Language (UML). \nAn important objective of modeling is to identify ambiguities in the requirements, as well as defining the scope of the implementation.\n\n\\subsection{ Differences between Modeling for Development and Modeling for testing}\nIt might be tempting to reuse the same models created by the development team for testing purposes. However this is not considered a good practice. Models created for developing tend to have way too much detail, especially regarding low-level implementation issues, which may not be relevant for testing. These models also tend to to not be very specific regarding data processing and manipulation, and focus instead on the structure of the SUT.\n\nThere are therefore two characteristics that a good model should satisfy for test generation: They must be small in comparison to the scope of the whole system, which reduces their cost and complexity, but should be detailed enough that they include all the characteristics relevant to the desired test. It is the job of a good test modeler to find this balance, and it is not very common to find models created for development that satisfy these conditions.\nFinally, using the same models as the development team would mean that any error injected in the design phase or in the requirements will persist, since the tests are going to be applied to the same incorrect model.\n\n\\section{The Model-Based testing process}\nThe process of creating the appropriate tests from a set of requirements consists on the following steps:\n\n\\subsection{Model Definition}\nFirst the level of abstraction should be decided. This is done with the objective of reducing the detail of the model as much as possible while still covering the test objectives. Next the data, operations, outputs and the interactions of the module with other systems or subsystems have to be considered. Unless different input parameters change the behavior of a particular operation, or produce a change in state, it is recommended to leave them out of the model for simplicity. Input parameters are better dealt with once the abstract tests have been generated and being translated into executable tests.\nOutputs of operations should only be included if their value can be used for the test oracle. \nThe behavior of the model can be different than the one of the SUT. Complex operations can be split into smaller steps for a more in depth test of the process. Sequence of operations in the SUT can also be aggregated into a single operation in the model if the result is the only aspect being tested.\n\nNext it is necessary to define the notation language that is going to be used to express the model. These can be classified into two major paradigms: Transition-based models and State-based (also known as pre/post) notations. A brief overview of each is given below.\n\nTransition based models: With this type of notation the SUT is represented as a series of states and the transitions between them. If determined correctly, each state in the model should result in a change in behavior of the system.Usually these models are expressed using nodes to represent the states and arrows to represent the different transitions. The most simple example of transition based models are finite state machines (FSMs), although there are other languages such as UML State Machines that allow to expand the models with variables, hierarchies or parallelism between machines. Transition based models are recommended for control-oriented systems.\n\nPre/Post Models: This type of notation models the SUT using variables to represent the internal state of the system and the operations that modify them. These operations are defined by preconditions and postconditions. Some of the most used examples of this paradigm are the B notation and UML Object Constraint Language (OCL).\nTransition based models are recommended for data oriented systems.\n\nFinally the model should be verified and validated. Verification concerns making sure the model has been created correctly, while validation checks that the model does represent the aspects of the SUT that need to be tested.\n\n\n\\subsection{Generation of abstract test cases}\n\nIt is in this stage that the tester should determine the appropriate selection criteria to generate the necessary tests from the model. A simple benchmark for test generation can be, for example, to generate tests such as all the requirements have been covered (Requirements-based criteria). Selection criteria can also be based on the coverage criteria usually used in white box testing, but applied in a prescriptive way. These can be subdivided into structural model criteria, which seek to achieve coverage of the control-flow, and data coverage criteria, which aim to cover the input data space of an operation or a transition.\n\nThe tester may also want to apply his previous experiences and his knowledge of the domain space to explicitly specify the set of tests that should be generated, or to focus the tests on a specific kind of fault. The most commonly used tools for test generation are flexible enough to allow this. Some f the most common are LEIRIOS Test Generator (Which generates tests from models that use the B notation), and Qtronic.\n\nIt is important to notice that the output of this stage is a suite of abstract test cases, meaning that they cannot be directly executed because they are not defined in any programming language. This, in turn, increases the re-usability and flexibility of the tests.\n\n\n\\subsection{Transformation into executable test scripts}\n<This section will be expanded further>>\nIn this section we will review the further work that needs to be done in order to transform an abstract test case into an executable one. There are tools that help with this process, but there’s still effort required to be done by the tester, including the appropriate selection of inputs.\n\nConcretization\n“The adaptation approach, (a), is to manually write some adapter code that bridges the gap. This is essentially a wrapper around the SUT that provides a more abstract view of the SUT to match the abstraction level of the model. · The transformation approach, (b), is to transform the abstract tests into concrete test scripts.\nThe mixed approach, (c), is a combination of the other two approaches. It is sometimes useful to add some adapter code around the SUT to raise its abstraction level part of the way toward the model and make testing easier, and then transform the abstract tests into a more concrete form that matches the adapter interface. Some benefits of this mixed approach are that the transformation can be easier, since the levels of abstraction are closer, and the adapter can be less model-specific, which may allow it to be reused for several different models or different testing scenarios.”\n\n“8.1.3 Which Approach Is Better? For online testing, it is almost always better to use the adaptation approach because online testing requires a tightly integrated, two-way connection be- tween the model-based testing tool and the SUT. This is easiest to achieve when the model-based testing can directly connect to the adapter API and the adapter is in turn directly connected to the SUT interface.\nThe transformation approach has the advantage that it can produce test scripts in the same language and with the same naming and structuring con- ventions that are already used for manually written tests. This can make the adoption of model-based testing less disruptive on the overall testing process by allowing existing test management tools, test repositories, test execution platforms, and test reporting processes to remain unchanged. Essentially, model-based testing is replacing the test design and test scripting stages of the testing process by new techniques and tools, but the rest of the testing process remains the same.”\n\n“Key Point For online testing, use the adaptation approach. For offline testing, the transformation approach has some advantages (less disruption), and it is often useful to combine it with the adaptation approach.”\n\n\n\\subsection{Execution of the tests and evaluation of the model}\n<<In this section we will expand on the concept of coverage satisfaction, and how models can help us find errors in the requirements>>\n\n\\section{Advantages of model based testing}\n“benefits of model-based testing In this section, we discuss the various benefits that model-based testing can have. We group these into six areas: SUT fault detection, reduced testing cost and time, improved test quality, requirements defect detection, traceability, and requirements evolution.”\n\n“Advantages of model based testing over other approaches:\n· Automation of the design of functional test cases (including generation of the expected results) to reduce the design cost and to produce test suites with systematic coverage of the model \n· Reduction of the maintenance costs of the test suite \n· Automatic generation of the traceability matrix from requirements to test cases”\n\n\\section{Organizational requirements for Successful Model-Based Testing implementations}\n\n\\section{Model-Based Testing in Agile Development environments}\n<<Based on a paper from David Faragó for the University of Karlsruhe, this section will talk about the new challenges that agile methodologies have generated in the world of software testing, and how the role of the tester has changed. The flexibility provided by using Model-Based testing has allowed to significantly reduce the time to ship.\n\n\\section{Limitations of MBT}\n\n\\section{Survey of tools}\n<<In this section we will provide a brief description of 5 tools currently in the market, the services they provide, their cost and we will search for customer satisfaction evaluations in an attempt to identify which are appropriate for different types of projects.\n\n\\subsection{fMBT}\n\\subsection{MaTeLo}\n\\subsection{Tosca}\n\\subsection{4test}\n\\subsection{Modbat}\n\n\n\\section{Conclusions}\n\n\n\n\\paragraph{Sample Heading (Fourth Level)}\nThe contribution should contain no more than four levels of\nheadings. Table~\\ref{tab1} gives a summary of all heading levels.\n\n\\begin{table}\n\\caption{Table captions should be placed above the\ntables.}\\label{tab1}\n\\begin{tabular}{|l|l|l|}\n\\hline\nHeading level &  Example & Font size and style\\\\\n\\hline\nTitle (centered) &  {\\Large\\bfseries Lecture Notes} & 14 point, bold\\\\\n1st-level heading &  {\\large\\bfseries 1 Introduction} & 12 point, bold\\\\\n2nd-level heading & {\\bfseries 2.1 Printing Area} & 10 point, bold\\\\\n3rd-level heading & {\\bfseries Run-in Heading in Bold.} Text follows & 10 point, bold\\\\\n4th-level heading & {\\itshape Lowest Level Heading.} Text follows & 10 point, italic\\\\\n\\hline\n\\end{tabular}\n\\end{table}\n\n\n\\noindent Displayed equations are centered and set on a separate\nline.\n\\begin{equation}\nx + y = z\n\\end{equation}\nPlease try to avoid rasterized images for line-art diagrams and\nschemas. Whenever possible, use vector graphics instead (see\nFig.~\\ref{fig1}).\n\n\\begin{figure}\n\\includegraphics[width=\\textwidth]{fig1.eps}\n\\caption{A figure caption is always placed below the illustration.\nPlease note that short captions are centered, while long ones are\njustified by the macro package automatically.} \\label{fig1}\n\\end{figure}\n\n\\begin{theorem}\nThis is a sample theorem. The run-in heading is set in bold, while\nthe following text appears in italics. Definitions, lemmas,\npropositions, and corollaries are styled the same way.\n\\end{theorem}\n%\n% the environments 'definition', 'lemma', 'proposition', 'corollary',\n% 'remark', and 'example' are defined in the LLNCS documentclass as well.\n%\n\\begin{proof}\nProofs, examples, and remarks have the initial word in italics,\nwhile the following text appears in normal font.\n\\end{proof}\nFor citations of references, we prefer the use of square brackets\nand consecutive numbers. Citations using labels or the author/year\nconvention are also acceptable. The following bibliography provides\na sample reference list with entries for journal\narticles~\\cite{ref_article1}, an LNCS chapter~\\cite{ref_lncs1}, a\nbook~\\cite{ref_book1}, proceedings without editors~\\cite{ref_proc1},\nand a homepage~\\cite{ref_url1}. Multiple citations are grouped\n\\cite{ref_article1,ref_lncs1,ref_book1},\n\\cite{ref_article1,ref_book1,ref_proc1,ref_url1}.\n%\n% ---- Bibliography ----\n%\n% BibTeX users should specify bibliography style 'splncs04'.\n% References will then be sorted and formatted in the correct style.\n%\n% \\bibliographystyle{splncs04}\n% \\bibliography{mybibliography}\n%\n\\begin{thebibliography}{8}\n\\bibitem{ref_article1}\nAuthor, F.: Article title. Journal \\textbf{2}(5), 99--110 (2016)\n\n\\bibitem{ref_lncs1}\nAuthor, F., Author, S.: Title of a proceedings paper. In: Editor,\nF., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.\nSpringer, Heidelberg (2016). \\doi{10.10007/1234567890}\n\n\\bibitem{ref_book1}\nAuthor, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,\nLocation (1999)\n\n\\bibitem{ref_proc1}\nAuthor, A.-B.: Contribution title. In: 9th International Proceedings\non Proceedings, pp. 1--2. Publisher, Location (2010)\n\n\\bibitem{ref_url1}\nLNCS Homepage, \\url{http://www.springer.com/lncs}. Last accessed 4\nOct 2017\n\\end{thebibliography}\n\\end{document}\n",
			"settings":
			{
				"buffer_size": 20206,
				"line_ending": "Unix",
				"name": "% This is samplepaper.tex, a sample chapter demons"
			}
		},
		{
			"contents": "% This is samplepaper.tex, a sample chapter demonstrating the\n% LLNCS macro package for Springer Computer Science proceedings;\n% Version 2.20 of 2017/10/04\n%\n\\documentclass[runningheads]{llncs}\n%\n\\usepackage{graphicx}\n% Used for displaying a sample figure. If possible, figure files should\n% be included in EPS format.\n%\n% If you use the hyperref package, please uncomment the following line\n% to display URLs in blue roman font according to Springer's eBook style:\n% \\renewcommand\\UrlFont{\\color{blue}\\rmfamily}\n\n\\begin{document}\n%\n\\title{An Overview of Model-Based Testing\\thanks{Supported by organization x.}}\n%\n%\\titlerunning{Abbreviated paper title}\n% If the paper title is too long for the running head, you can set\n% an abbreviated paper title here\n%\n\\author{Jorge Villalobos Rebollosa\\inst{1}\\orcidID{0000-1111-2222-3333} \\and\nSecond Author\\inst{2,3}\\orcidID{1111-2222-3333-4444} \\and\nThird Author\\inst{3}\\orcidID{2222--3333-4444-5555}}\n%\n\n\\authorrunning{F. Author et al.}\n% First names are abbreviated in the running head.\n% If there are more than two authors, 'et al.' is used.\n%\n\\institute{University of Mannheim, Mannheim, Germany }\n%\n\\maketitle              % typeset the header of the contribution\n%\n\\begin{abstract}\nThe purpose of the following document is to provide an overview of  \nModel-Based Testing (MBT), highlighting its differences with other testing approaches and especially focusing on its application alongside agile development methodologies. \nThe document will then explain how model-based testing facilitates the automation of test case generation, and how it relates to model-driven development and constraint programming.\nThe advantages and challenges of this testing methodology will be exposed, and a summary and evaluation of tools for its implementation will be provided.\n\n\n\\keywords{Testing  \\and Modeling \\and Model-Based Testing \\and Agile Development}\n\\end{abstract}\n%\n%\n%\n\\section{Introduction}\nWith the increasing adoption of Agile development methodologies and the shortening of the development cycles it has become imperative for software testers to find  better and more efficient ways to evaluate the quality of software products. The automation of test execution has reduced significantly the time required for testing while also increasing the re-usability of the generated test suite, but still it depends on the tester to generate the right test cases that satisfy the quality requirements for the system or module. This has led to the development of approaches that seek to automate the test generation process, the most common of which is known as Model-based testing.\n\\section{Model-Based Testing}\n\n\\subsection{The testing process}\n\nThe most basic task of the software tester is to generate a set of test cases (known also as Test Suite) and then run those tests in the System Under Test (SUT) in order to evaluate the quality of the system against the informal requirements. For the tester to be able to evaluate whether a test passed successfully or not, a Test Oracle is required, which is a list, method or system that provides the correct results or behavior of a system in response to a specific set of inputs. \n\nIdeally the tester will create an exhaustive set of tests that go through every aspect of the program and verify that every possible input produces the required state. In reality though, this is impossible, since the amount of tests could be infinite. Moreover, systems are developed under strict  time and budget deadlines, so only a fractional amount of the possible test cases can be executed before release. It is therefore the job of the tester to determine which set of test cases should be executed, and to provide information on which parts of the overall system have been evaluated, and which parts haven’t. \n\nThe most resource consuming part of the testing process is the execution of the previously generated tests. This is also a boring and repetitive task, and can also be very expensive since it requires a lot of human effort. It is also prone to human errors. To tackle this several test automation tools exist in the market. The goal of test automation is for the tester to use a programming language to define the test once, and then execute these test automatically whenever necessary. This allows for concepts such as Continuous Integration to be possible, since developers can now automatically trigger the regression test suite every time they push a new version of the module they are working on.\n\nHowever, the test automation of the test execution still depends on the tester to design the test suite, which means there is still the need for a test oracle, and there is still the problem of generating the right tests to ensure the desired coverage. Automated test suites also have the disadvantage of not being very flexible, since small changes on the way the system works (A change in the system’s API for example) renders a lot of tests useless.\n\n\\subsection{Model-Based Testing}\n\nNew testing approaches allow us to  take automation one step further. Instead of having the tester generate the test suite by himself, the role of the tester is now to determine an abstract version of the system based on the requirements, and then allowing tools to automatically generate a set of tests from that version. This allows the tester to automatically generate a set of tests that satisfy the desired coverage. This new role also allows the tester to perform an evaluation of the informal requirements, so ambiguous or incomplete requirements can also be quickly found. As a result, the testing process can now begin before a single line of code is written by the developers, and finding errors in the requirement gathering/design phase of the software development life cycle ends up saving a lot of time and money. This is where the Model-based testing comes into play.\n\nModel-Based Testing can therefore be defined as the “automation of the design of black box tests”. It is an application of concepts derived from the more general Model Driven Development (MDD), and can take advantage of most of the same (or similar) modeling languages. \n\n\\subsection{ Modeling for testing}\nEvery system developed has as a starting point a set of informal requirements, provided by the client or one of the shareholders. The client usually has no knowledge of how a system is built, and as such the requirements are usually expressed in a non technical language, in terms of desired results or functionality. Therefore, before a system can be built it is necessary to transform this requirements into something the developers can actually use to begin building the system. Models provide a solution for this process. Software models are abstractions of the required functionalities of a system, taking advantage of visual techniques or an abstract language to represent the intended design.\nThere are several object modeling languages and techniques that can be used to create these models, one of the most common being the Universal Modeling Language (UML). \nAn important objective of modeling is to identify ambiguities in the requirements, as well as defining the scope of the implementation.\n\n\\subsection{ Differences between Modeling for Development and Modeling for testing}\nIt might be tempting to reuse the same models created by the development team for testing purposes. However this is not considered a good practice. Models created for developing tend to have way too much detail, especially regarding low-level implementation issues, which may not be relevant for testing. These models also tend to to not be very specific regarding data processing and manipulation, and focus instead on the structure of the SUT.\n\nThere are therefore two characteristics that a good model should satisfy for test generation: They must be small in comparison to the scope of the whole system, which reduces their cost and complexity, but should be detailed enough that they include all the characteristics relevant to the desired test. It is the job of a good test modeler to find this balance, and it is not very common to find models created for development that satisfy these conditions.\nFinally, using the same models as the development team would mean that any error injected in the design phase or in the requirements will persist, since the tests are going to be applied to the same incorrect model.\n\n\\section{The Model-Based testing process}\nThe process of creating the appropriate tests from a set of requirements consists on the following steps:\n\n\\subsection{Model Definition}\nFirst the level of abstraction should be decided. This is done with the objective of reducing the detail of the model as much as possible while still covering the test objectives. Next the data, operations, outputs and the interactions of the module with other systems or subsystems have to be considered. Unless different input parameters change the behavior of a particular operation, or produce a change in state, it is recommended to leave them out of the model for simplicity. Input parameters are better dealt with once the abstract tests have been generated and being translated into executable tests.\nOutputs of operations should only be included if their value can be used for the test oracle. \nThe behavior of the model can be different than the one of the SUT. Complex operations can be split into smaller steps for a more in depth test of the process. Sequence of operations in the SUT can also be aggregated into a single operation in the model if the result is the only aspect being tested.\n\nNext it is necessary to define the notation language that is going to be used to express the model. These can be classified into two major paradigms: Transition-based models and State-based (also known as pre/post) notations. A brief overview of each is given below.\n\n\\subsubsection{Transition based models}\n With this type of notation the SUT is represented as a series of states and the transitions between them. If determined correctly, each state in the model should result in a change in behavior of the system.Usually these models are expressed using nodes to represent the states and arrows to represent the different transitions. The most simple example of transition based models are finite state machines (FSMs), although there are other languages such as UML State Machines that allow to expand the models with variables, hierarchies or parallelism between machines. Transition based models are recommended for control-oriented systems.\n\n\\subsubsection{Pre/Post Models}\nThis type of notation models the SUT using variables to represent the internal state of the system and the operations that modify them. These operations are defined by preconditions and postconditions. Some of the most used examples of this paradigm are the B notation and UML Object Constraint Language (OCL).Pre/Post models are recommended for data oriented systems.\n\n\nFinally the model should be verified and validated. Verification concerns making sure the model has been created correctly, while validation checks that the model does represent the aspects of the SUT that need to be tested.\n\n\n\\subsection{Generation of abstract test cases}\n\nIt is in this stage that the tester should determine the appropriate selection criteria to generate the necessary tests from the model. \n\n\\subsubsection{Coverage Criteria}\n\nA simple benchmark for test generation can be, for example, to generate tests such as all the requirements have been covered (Requirements-based criteria). Selection criteria can also be based on the coverage criteria usually used in white box testing, but applied in a prescriptive way. The three most relevant types of coverage ate Statement Coverage (SC), Decision Coverage (DC) and Path Coverage (PC).To achieve statement coverage, every single state within the system should be reached at one point by the tests. Decision Coverage, as it name suggests, is concerned with every decision, and to achieve it the tests must follow every option on every decision at least once. Satisfying DC also implies satisfying SC.Finally, the strongest criteria, Path Coverage, requires every single satisfiable path to be traversed by the tests.\n These can be subdivided into structural model criteria, which seek to achieve coverage of the control-flow, and data coverage criteria, which aim to cover the input data space of an operation or a transition.\n\n\\subsubsection{Domain Specific and Statistical Criteria}\nThe tester may also want to apply his previous experiences and his knowledge of the domain space to explicitly specify the set of tests that should be generated, or to focus the tests on a specific kind of fault. Finally, random generation can also be used as a criteria, especially when there is a limited amount of time and a general overview of the behaviors of the system is desired.\n\n\\subsubsection{Selecting the Right Criteria}\nThe amount of tests generated, and the time it will take the tool to do so are influenced on the selected criteria.It is therefore recommended to select different selection criteria depending on the part of the SUT that is being tested. The most commonly used tools for test generation are flexible enough to allow this.  Some examples are LEIRIOS Test Generator (Which generates tests from models that use the B notation), and Qtronic.\nStandards and regulations regarding the minimum of coverage required for some specific domain spaces should also be considered.\n\n\n\n\\subsection{Concretization}\nAt this stage it is assumed that a working version of the part of the SUT that needs to be tested is already created and initialized.\n\nThe output of the test generation stage is a suite of abstract test cases, meaning that they cannot be directly executed in the SUT. This increases the reusability and flexibility of the tests, but creates the necessity of bridging the gap between the abstract model and the SUT. \n\nIn this section, the further work that needs to be done in order to transform an abstract test case into an executable one will be reviewed. This process is known as concretization. \nThere are two main approaches for concretization of a test suite: the adaptation approach and the transformation approach. \n\n\\subsubsection{The Adaptation Approach}\n\nThe adaptation approach involves creating an adapter code that interprets the operations of the model and executes them in the SUT. First, model-level operations and abstract input values are converted into concrete SUT operations. These are then executed in the SUT and the outputs (or any other relevant results) are captured. These are then converted back again into abstract values, which are then fed back into the model. The model’s rules then are used to determine whether the test passed or not. These transformations are done in real time, as the abstract test suite is interpreted. The adaptation approach is recommended when dealing with online testing, where the adapter’s API can provided a tighty integrated two-way connection between the SUT interface and the model.\n\n\\subsection{The Transformation Approach}\n\nThe transformation approach involves creating executable test scripts out of the abstract tests. These can be created in standard programming or test scripting languages. These tests can then be executed and evaluated by the testing team. The transformation approach can be very useful for organizations that already have a mature testing process and therefore have already defined an automated test execution framework. The right tool can then be used to adapt the tests created by the model directly into the existent framework. These generated tests can follow the same organization and version management standards currently defined. In this case the adoption of model-based testing would be less disruptive, since the current test management tools, repositories, test execution platforms, and test reporting processes don’t have to be changed.\nHowever, if the organization’s processes are not mature enough then it is very important to first define an appropriate structure to the test suite, so the hundreds (or even thousands) of generated tests can be tracked. Naming conventions, folders and hierarchies should be applied. Treaceability from the test scripts, the abstract tests and the original requirements must also be maintained.\n\nA third option is to apply a mix of the two approaches. An adapter code can be added to the SUT with the purpose of raising the abstraction level, bringing it closer to the abstraction of the model and making the transformation easier.\n\n\\subsection{Execution and Analysis}\nFinally the tests can be executed and the results can be analyzed. Some model-based testing tools can also handle this phase, or organizations can use the tools they already have for managing the execution and documentation of test scripts.  It is important to note that a failed test can also be caused due to an error in the adapter or in the model itself. During the initial runs of the test suite a lot of feedback about the correctness of the model is obtained, and a higher amount of errors are expected. The testers should then use this feedback to improve the quality of the model, correct adapter errors or even identify flaws in the requirements.\n\n\\section{Advantages of model based testing}\nIn this section the advantages that adopting model-based testing can bring to organizations will be discussed.\n\nReduced cost and time\nModel-based testing can have a significant impact in the testing time if the effort to create and maintain the model is lower than the effort directed to manually create the test suite. There are also several time saving advantages after the tests have been executed, during the analysis and debugging phase. Generated tests report failures in a more consistent way and some tools provide the capability of finding the shortest test sequence that produces a failure, which makes debugging easier. A successful model-based testing implementation will result in the automation of the majority of the testing process, and subsequently results in a decrease in development costs.\n\n\n\n\\subsection{Improved test quality}\n\nManually deriving test cases from requirements is a laborious and error-prone process whose success depends on the ingenuity of the testers. Manually scripted tests are also software programs and are therefore also vulnerable to error injection, meaning that the test scripts themselves have to be tested as well. Model-based testing approaches therefore produce more consistent and reliable tests.\nManually creating a test suite that satisfies the desired coverage can be almost impossible, since even simple programs can involve a very large amount of paths. An industry survey by CA technologies in 2015 found that over 42 percent of the participants “cited a lack of test coverage creating defects and rework as a main software challenge.”\nModel-based testing also produces more optimized tests, and the resulting test suite usually contains the smallest amount of paths needed for maximum coverage. \n\n\n\\subsection{Requirements defect detection}\n\nWriting a model can also expose issues with the requirements. Incomplete, ambiguous or contradictory requirements become more apparent when translated to the precise semantics required by the model. Errors found in the requirements will not be carried over to the design and implementation phases. This is a major benefit since the cost of fixing errors in software products increases exponentially by every phase of the development process. Figure xxx illustrates this increase.\n\n\\subsection{Traceability}\n\nModel-based testing also provides the ability to quickly relate every test to the model and to the requirements.This is called a traceability matrix. If requirements change and the model evolves, traceability becomes very useful for optimization since it allows the tester to focus on the execution of the tests related to the changes.\n\n\n\n\\section{Organizational requirements for Successful Model-Based Testing implementations}\n\n\\section{Model-Based Testing in Agile Development environments}\n<<Based on a paper from David Faragó for the University of Karlsruhe, this section will talk about the new challenges that agile methodologies have generated in the world of software testing, and how the role of the tester has changed. The flexibility provided by using Model-Based testing has allowed to significantly reduce the time to ship.\n\n\\section{Limitations of MBT}\n\n\\section{Survey of tools}\n<<In this section we will provide a brief description of 5 tools currently in the market, the services they provide, their cost and we will search for customer satisfaction evaluations in an attempt to identify which are appropriate for different types of projects.\n\n\\subsection{fMBT}\n\\subsection{MaTeLo}\n\\subsection{Tosca}\n\\subsection{4test}\n\\subsection{Modbat}\n\n\n\\section{Conclusions}\n\n\n\nFor citations of references, we prefer the use of square brackets\nand consecutive numbers. Citations using labels or the author/year\nconvention are also acceptable. The following bibliography provides\na sample reference list with entries for journal\narticles~\\cite{ref_article1}, an LNCS chapter~\\cite{ref_lncs1}, a\nbook~\\cite{ref_book1}, proceedings without editors~\\cite{ref_proc1},\nand a homepage~\\cite{ref_url1}. Multiple citations are grouped\n\\cite{ref_article1,ref_lncs1,ref_book1},\n\\cite{ref_article1,ref_book1,ref_proc1,ref_url1}.\n%\n% ---- Bibliography ----\n%\n% BibTeX users should specify bibliography style 'splncs04'.\n% References will then be sorted and formatted in the correct style.\n%\n% \\bibliographystyle{splncs04}\n% \\bibliography{mybibliography}\n%\n\\begin{thebibliography}{8}\n\\bibitem{ref_article1}\nAuthor, F.: Article title. Journal \\textbf{2}(5), 99--110 (2016)\n\n\\bibitem{ref_lncs1}\nAuthor, F., Author, S.: Title of a proceedings paper. In: Editor,\nF., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.\nSpringer, Heidelberg (2016). \\doi{10.10007/1234567890}\n\n\\bibitem{ref_book1}\nAuthor, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,\nLocation (1999)\n\n\\bibitem{ref_proc1}\nAuthor, A.-B.: Contribution title. In: 9th International Proceedings\non Proceedings, pp. 1--2. Publisher, Location (2010)\n\n\\bibitem{ref_url1}\nLNCS Homepage, \\url{http://www.springer.com/lncs}. Last accessed 4\nOct 2017\n\\end{thebibliography}\n\\end{document}\n",
			"settings":
			{
				"buffer_size": 22376,
				"line_ending": "Unix",
				"name": "% This is samplepaper.tex, a sample chapter demons"
			}
		},
		{
			"contents": "% This is samplepaper.tex, a sample chapter demonstrating the\n% LLNCS macro package for Springer Computer Science proceedings;\n% Version 2.20 of 2017/10/04\n%\n\\documentclass[runningheads]{llncs}\n%\n\\usepackage{graphicx}\n% Used for displaying a sample figure. If possible, figure files should\n% be included in EPS format.\n%\n% If you use the hyperref package, please uncomment the following line\n% to display URLs in blue roman font according to Springer's eBook style:\n% \\renewcommand\\UrlFont{\\color{blue}\\rmfamily}\n\n\\begin{document}\n%\n\\title{An Overview of Model-Based Testing\\thanks{Supported by organization x.}}\n%\n%\\titlerunning{Abbreviated paper title}\n% If the paper title is too long for the running head, you can set\n% an abbreviated paper title here\n%\n\\author{Jorge Villalobos Rebollosa\\inst{1}\\orcidID{0000-1111-2222-3333}}\n%\n\n\n% First names are abbreviated in the running head.\n% If there are more than two authors, 'et al.' is used.\n%\n\\institute{University of Mannheim, Mannheim, Germany }\n%\n\\maketitle              % typeset the header of the contribution\n%\n\\begin{abstract}\nThe purpose of the following document is to provide an overview of  \nModel-Based Testing (MBT), highlighting its differences with other testing approaches and especially focusing on its application alongside agile development methodologies. \nThe document will then explain how model-based testing facilitates the automation of test case generation, and how it relates to model-driven development and constraint programming.\nThe advantages and challenges of this testing methodology will be exposed, and a summary and evaluation of tools for its implementation will be provided.\n\n\n\\keywords{Testing  \\and Modeling \\and Model-Based Testing \\and Agile Development}\n\\end{abstract}\n%\n%\n%\n\\section{Introduction}\nWith the increasing adoption of Agile development methodologies and the shortening of the development cycles it has become imperative for software testers to find  better and more efficient ways to evaluate the quality of software products. The automation of test execution has reduced significantly the time required for testing while also increasing the re-usability of the generated test suite, but still it depends on the tester to generate the right test cases that satisfy the quality requirements for the system or module. This has led to the development of approaches that seek to automate the test generation process, the most common of which is known as Model-based testing.\n\\section{Model-Based Testing}\n\n\\subsection{The testing process}\n\nThe most basic task of the software tester is to generate a set of test cases (known also as Test Suite) and then run those tests in the System Under Test (SUT) in order to evaluate the quality of the system against the informal requirements. For the tester to be able to evaluate whether a test passed successfully or not, a Test Oracle is required, which is a list, method or system that provides the correct results or behavior of a system in response to a specific set of inputs. \n\nIdeally the tester will create an exhaustive set of tests that go through every aspect of the program and verify that every possible input produces the required state. In reality though, this is impossible, since the amount of tests could be infinite. Moreover, systems are developed under strict  time and budget deadlines, so only a fractional amount of the possible test cases can be executed before release. It is therefore the job of the tester to determine which set of test cases should be executed, and to provide information on which parts of the overall system have been evaluated, and which parts haven’t. \n\nThe most resource consuming part of the testing process is the execution of the previously generated tests. This is also a boring and repetitive task, and can also be very expensive since it requires a lot of human effort. It is also prone to human errors. To tackle this several test automation tools exist in the market. The goal of test automation is for the tester to use a programming language to define the test once, and then execute these test automatically whenever necessary. This allows for concepts such as Continuous Integration to be possible, since developers can now automatically trigger the regression test suite every time they push a new version of the module they are working on.\n\nHowever, the test automation of the test execution still depends on the tester to design the test suite, which means there is still the need for a test oracle, and there is still the problem of generating the right tests to ensure the desired coverage. Automated test suites also have the disadvantage of not being very flexible, since small changes on the way the system works (A change in the system’s API for example) renders a lot of tests useless.\n\n\\subsection{Model-Based Testing}\n\nNew testing approaches allow us to  take automation one step further. Instead of having the tester generate the test suite by himself, the role of the tester is now to determine an abstract version of the system based on the requirements, and then allowing tools to automatically generate a set of tests from that version. This allows the tester to automatically generate a set of tests that satisfy the desired coverage. This new role also allows the tester to perform an evaluation of the informal requirements, so ambiguous or incomplete requirements can also be quickly found. As a result, the testing process can now begin before a single line of code is written by the developers, and finding errors in the requirement gathering/design phase of the software development life cycle ends up saving a lot of time and money. This is where the Model-based testing comes into play.\n\nModel-Based Testing can therefore be defined as the “automation of the design of black box tests”. It is an application of concepts derived from the more general Model Driven Development (MDD), and can take advantage of most of the same (or similar) modeling languages. \n\n\\subsection{ Modeling for testing}\nEvery system developed has as a starting point a set of informal requirements, provided by the client or one of the shareholders. The client usually has no knowledge of how a system is built, and as such the requirements are usually expressed in a non technical language, in terms of desired results or functionality. Therefore, before a system can be built it is necessary to transform this requirements into something the developers can actually use to begin building the system. Models provide a solution for this process. Software models are abstractions of the required functionalities of a system, taking advantage of visual techniques or an abstract language to represent the intended design.\nThere are several object modeling languages and techniques that can be used to create these models, one of the most common being the Universal Modeling Language (UML). \nAn important objective of modeling is to identify ambiguities in the requirements, as well as defining the scope of the implementation.\n\n\\subsection{ Differences between Modeling for Development and Modeling for testing}\nIt might be tempting to reuse the same models created by the development team for testing purposes. However this is not considered a good practice. Models created for developing tend to have way too much detail, especially regarding low-level implementation issues, which may not be relevant for testing. These models also tend to to not be very specific regarding data processing and manipulation, and focus instead on the structure of the SUT.\n\nThere are therefore two characteristics that a good model should satisfy for test generation: They must be small in comparison to the scope of the whole system, which reduces their cost and complexity, but should be detailed enough that they include all the characteristics relevant to the desired test. It is the job of a good test modeler to find this balance, and it is not very common to find models created for development that satisfy these conditions.\nFinally, using the same models as the development team would mean that any error injected in the design phase or in the requirements will persist, since the tests are going to be applied to the same incorrect model.\n\n\\section{The Model-Based testing process}\nThe process of creating the appropriate tests from a set of requirements consists on the following steps:\n\n\\subsection{Model Definition}\nFirst the level of abstraction should be decided. This is done with the objective of reducing the detail of the model as much as possible while still covering the test objectives. Next the data, operations, outputs and the interactions of the module with other systems or subsystems have to be considered. Unless different input parameters change the behavior of a particular operation, or produce a change in state, it is recommended to leave them out of the model for simplicity. Input parameters are better dealt with once the abstract tests have been generated and being translated into executable tests.\nOutputs of operations should only be included if their value can be used for the test oracle. \nThe behavior of the model can be different than the one of the SUT. Complex operations can be split into smaller steps for a more in depth test of the process. Sequence of operations in the SUT can also be aggregated into a single operation in the model if the result is the only aspect being tested.\n\nNext it is necessary to define the notation language that is going to be used to express the model. These can be classified into two major paradigms: Transition-based models and State-based (also known as pre/post) notations. A brief overview of each is given below.\n\n\\subsubsection{Transition based models}\n With this type of notation the SUT is represented as a series of states and the transitions between them. If determined correctly, each state in the model should result in a change in behavior of the system.Usually these models are expressed using nodes to represent the states and arrows to represent the different transitions. The most simple example of transition based models are finite state machines (FSMs), although there are other languages such as UML State Machines that allow to expand the models with variables, hierarchies or parallelism between machines. Transition based models are recommended for control-oriented systems.\n\n\\subsubsection{Pre/Post Models}\nThis type of notation models the SUT using variables to represent the internal state of the system and the operations that modify them. These operations are defined by preconditions and postconditions. Some of the most used examples of this paradigm are the B notation and UML Object Constraint Language (OCL).Pre/Post models are recommended for data oriented systems.\n\n\nFinally the model should be verified and validated. Verification concerns making sure the model has been created correctly, while validation checks that the model does represent the aspects of the SUT that need to be tested.\n\n\n\\subsection{Generation of abstract test cases}\n\nIt is in this stage that the tester should determine the appropriate selection criteria to generate the necessary tests from the model. \n\n\\subsubsection{Coverage Criteria}\n\nA simple benchmark for test generation can be, for example, to generate tests such as all the requirements have been covered (Requirements-based criteria). Selection criteria can also be based on the coverage criteria usually used in white box testing, but applied in a prescriptive way. The three most relevant types of coverage ate Statement Coverage (SC), Decision Coverage (DC) and Path Coverage (PC).To achieve statement coverage, every single state within the system should be reached at one point by the tests. Decision Coverage, as it name suggests, is concerned with every decision, and to achieve it the tests must follow every option on every decision at least once. Satisfying DC also implies satisfying SC.Finally, the strongest criteria, Path Coverage, requires every single satisfiable path to be traversed by the tests.\n These can be subdivided into structural model criteria, which seek to achieve coverage of the control-flow, and data coverage criteria, which aim to cover the input data space of an operation or a transition.\n\n\\subsubsection{Domain Specific and Statistical Criteria}\nThe tester may also want to apply his previous experiences and his knowledge of the domain space to explicitly specify the set of tests that should be generated, or to focus the tests on a specific kind of fault. Finally, random generation can also be used as a criteria, especially when there is a limited amount of time and a general overview of the behaviors of the system is desired.\n\n\\subsubsection{Selecting the Right Criteria}\nThe amount of tests generated, and the time it will take the tool to do so are influenced on the selected criteria.It is therefore recommended to select different selection criteria depending on the part of the SUT that is being tested. The most commonly used tools for test generation are flexible enough to allow this.  Some examples are LEIRIOS Test Generator (Which generates tests from models that use the B notation), and Qtronic.\nStandards and regulations regarding the minimum of coverage required for some specific domain spaces should also be considered.\n\n\n\n\\subsection{Concretization}\nAt this stage it is assumed that a working version of the part of the SUT that needs to be tested is already created and initialized.\n\nThe output of the test generation stage is a suite of abstract test cases, meaning that they cannot be directly executed in the SUT. This increases the reusability and flexibility of the tests, but creates the necessity of bridging the gap between the abstract model and the SUT. \n\nIn this section, the further work that needs to be done in order to transform an abstract test case into an executable one will be reviewed. This process is known as concretization. \nThere are two main approaches for concretization of a test suite: the adaptation approach and the transformation approach. \n\n\\subsubsection{The Adaptation Approach}\n\nThe adaptation approach involves creating an adapter code that interprets the operations of the model and executes them in the SUT. First, model-level operations and abstract input values are converted into concrete SUT operations. These are then executed in the SUT and the outputs (or any other relevant results) are captured. These are then converted back again into abstract values, which are then fed back into the model. The model’s rules then are used to determine whether the test passed or not. These transformations are done in real time, as the abstract test suite is interpreted. The adaptation approach is recommended when dealing with online testing, where the adapter’s API can provided a tighty integrated two-way connection between the SUT interface and the model.\n\n\\subsection{The Transformation Approach}\n\nThe transformation approach involves creating executable test scripts out of the abstract tests. These can be created in standard programming or test scripting languages. These tests can then be executed and evaluated by the testing team. The transformation approach can be very useful for organizations that already have a mature testing process and therefore have already defined an automated test execution framework. The right tool can then be used to adapt the tests created by the model directly into the existent framework. These generated tests can follow the same organization and version management standards currently defined. In this case the adoption of model-based testing would be less disruptive, since the current test management tools, repositories, test execution platforms, and test reporting processes don’t have to be changed.\nHowever, if the organization’s processes are not mature enough then it is very important to first define an appropriate structure to the test suite, so the hundreds (or even thousands) of generated tests can be tracked. Naming conventions, folders and hierarchies should be applied. Treaceability from the test scripts, the abstract tests and the original requirements must also be maintained.\n\nA third option is to apply a mix of the two approaches. An adapter code can be added to the SUT with the purpose of raising the abstraction level, bringing it closer to the abstraction of the model and making the transformation easier.\n\n\\subsection{Execution and Analysis}\nFinally the tests can be executed and the results can be analyzed. Some model-based testing tools can also handle this phase, or organizations can use the tools they already have for managing the execution and documentation of test scripts.  It is important to note that a failed test can also be caused due to an error in the adapter or in the model itself. During the initial runs of the test suite a lot of feedback about the correctness of the model is obtained, and a higher amount of errors are expected. The testers should then use this feedback to improve the quality of the model, correct adapter errors or even identify flaws in the requirements.\n\n\\section{Advantages of model based testing}\nIn this section the advantages that adopting model-based testing can bring to organizations will be discussed.\n\nReduced cost and time\nModel-based testing can have a significant impact in the testing time if the effort to create and maintain the model is lower than the effort directed to manually create the test suite. There are also several time saving advantages after the tests have been executed, during the analysis and debugging phase. Generated tests report failures in a more consistent way and some tools provide the capability of finding the shortest test sequence that produces a failure, which makes debugging easier. A successful model-based testing implementation will result in the automation of the majority of the testing process, and subsequently results in a decrease in development costs.\n\n\n\n\\subsection{Improved test quality}\n\nManually deriving test cases from requirements is a laborious and error-prone process whose success depends on the ingenuity of the testers. Manually scripted tests are also software programs and are therefore also vulnerable to error injection, meaning that the test scripts themselves have to be tested as well. Model-based testing approaches therefore produce more consistent and reliable tests.\nManually creating a test suite that satisfies the desired coverage can be almost impossible, since even simple programs can involve a very large amount of paths. An industry survey by CA technologies in 2015 found that over 42 percent of the participants “cited a lack of test coverage creating defects and rework as a main software challenge.”\nModel-based testing also produces more optimized tests, and the resulting test suite usually contains the smallest amount of paths needed for maximum coverage. \n\n\n\\subsection{Requirements defect detection}\n\nWriting a model can also expose issues with the requirements. Incomplete, ambiguous or contradictory requirements become more apparent when translated to the precise semantics required by the model. Errors found in the requirements will not be carried over to the design and implementation phases. This is a major benefit since the cost of fixing errors in software products increases exponentially by every phase of the development process. Figure xxx illustrates this increase.\n\n\\subsection{Traceability}\n\nModel-based testing also provides the ability to quickly relate every test to the model and to the requirements.This is called a traceability matrix. If requirements change and the model evolves, traceability becomes very useful for optimization since it allows the tester to focus on the execution of the tests related to the changes.\n\n\n\n\\section{Organizational requirements for Successful Model-Based Testing implementations}\nThe implementation of Model-Based testing practices in an organization will imply a significant change in paradigm in the way the organization approaches Software quality, and will most likely involve additional costs in personnel training, tool acquisitions and process re-engineering. Therefore, in order to increase the probability of a successful MBT implementation some factors should be considered. The first pre-requisite is for the organization to have in place a mature and well defined software development process, with well established channels of communication between the testing team and the rest of the project’s shareholders. \n\n\\subsection{The Test Maturity Model}\n\nIn order to assess the testing maturity level of organizations, the Illinois Institute of Technology developed the Test Maturity Model, which is based on the Capability Maturity Model but focuses on the testing process and the monitoring of quality. It is also divided into five levels of maturity. They are the following:\n\nLevel 1: Initial The organization lacks resources, tools, and trained staff.\n\nLevel 2: Phase Definition: Basic testing methods and techniques are in place.\n\nLevel 3: Integration: Testing is integrated into an entire life cycle, and the roles of tester and developer are separated.\n\nLevel 4: Management and Measurement Testing is a measured and quantified process, defects are logged and given severity levels.\n\nLevel 5: Optimized Testing is continuously improved, the organization invests in tools to support the design of test cases and the administration of defects.\n\nA level 5 Test maturity level is required for a Model-Based testing implementation, since it involves the adoption of new tools and the improvement of the testing process. It is also recommended that the organization has already in place a test execution automation process that can be adapted to work with the models.\n\n\\subsection{The Modeling Maturity Level}\n\nAn organization may also want to considered their current Modeling Maturity Level, especially with the aim of determining correctly the appropriate amount of investment that has to be made on personnel training. It consists on five levels:\n\nLevel 0: No Specification.\n\nLevel 1: Textual:Consists of informal, natural-language documents.\n\nLevel 2: Text with Diagrams: High level diagrams complement the text specifications.\nLevel 3:  Models with Text.\n\nLevel 4:  Precise Models: Recommended level for Model Driven Architecture approaches. The model has a precise meaning, and some code is automatically generated from the model.\n\nLevel 5:  Models Only:the model is used like a high-level programming language.\n\nSince the models created for MBT don’t have to be as detailed as the ones for MDT, a level 3 maturity should be enough for an organization to feel confident in their implementation.\n\n\\section{Model-Based Testing in Agile Development environments}\n<<Based on a paper from David Faragó for the University of Karlsruhe, this section will talk about the new challenges that agile methodologies have generated in the world of software testing, and how the role of the tester has changed. The flexibility provided by using Model-Based testing has allowed to significantly reduce the time to ship.\n\n\\section{Limitations of MBT}\n\n\\section{Survey of tools}\n<<In this section we will provide a brief description of 5 tools currently in the market, the services they provide, their cost and we will search for customer satisfaction evaluations in an attempt to identify which are appropriate for different types of projects.\n\n\\subsection{fMBT}\n\\subsection{MaTeLo}\n\\subsection{Tosca}\n\\subsection{4test}\n\\subsection{Modbat}\n\n\n\\section{Conclusions}\n\n\n\nFor citations of references, we prefer the use of square brackets\nand consecutive numbers. Citations using labels or the author/year\nconvention are also acceptable. The following bibliography provides\na sample reference list with entries for journal\narticles~\\cite{ref_article1}, an LNCS chapter~\\cite{ref_lncs1}, a\nbook~\\cite{ref_book1}, proceedings without editors~\\cite{ref_proc1},\nand a homepage~\\cite{ref_url1}. Multiple citations are grouped\n\\cite{ref_article1,ref_lncs1,ref_book1},\n\\cite{ref_article1,ref_book1,ref_proc1,ref_url1}.\n%\n% ---- Bibliography ----\n%\n% BibTeX users should specify bibliography style 'splncs04'.\n% References will then be sorted and formatted in the correct style.\n%\n% \\bibliographystyle{splncs04}\n% \\bibliography{mybibliography}\n%\n\\begin{thebibliography}{8}\n\\bibitem{ref_article1}\nAuthor, F.: Article title. Journal \\textbf{2}(5), 99--110 (2016)\n\n\\bibitem{ref_lncs1}\nAuthor, F., Author, S.: Title of a proceedings paper. In: Editor,\nF., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.\nSpringer, Heidelberg (2016). \\doi{10.10007/1234567890}\n\n\\bibitem{ref_book1}\nAuthor, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,\nLocation (1999)\n\n\\bibitem{ref_proc1}\nAuthor, A.-B.: Contribution title. In: 9th International Proceedings\non Proceedings, pp. 1--2. Publisher, Location (2010)\n\n\\bibitem{ref_url1}\nLNCS Homepage, \\url{http://www.springer.com/lncs}. Last accessed 4\nOct 2017\n\\end{thebibliography}\n\\end{document}\n",
			"settings":
			{
				"buffer_size": 25060,
				"line_ending": "Unix",
				"name": "% This is samplepaper.tex, a sample chapter demons"
			}
		},
		{
			"contents": "% This is samplepaper.tex, a sample chapter demonstrating the\n% LLNCS macro package for Springer Computer Science proceedings;\n% Version 2.20 of 2017/10/04\n%\n\\documentclass[runningheads]{llncs}\n%\n\\usepackage{graphicx}\n% Used for displaying a sample figure. If possible, figure files should\n% be included in EPS format.\n%\n% If you use the hyperref package, please uncomment the following line\n% to display URLs in blue roman font according to Springer's eBook style:\n% \\renewcommand\\UrlFont{\\color{blue}\\rmfamily}\n\n\\begin{document}\n%\n\\title{An Overview of Model-Based Testing\\thanks{Supported by organization x.}}\n%\n%\\titlerunning{Abbreviated paper title}\n% If the paper title is too long for the running head, you can set\n% an abbreviated paper title here\n%\n\\author{Jorge Villalobos Rebollosa\\inst{1}\\orcidID{0000-1111-2222-3333}}\n%\n\n\n% First names are abbreviated in the running head.\n% If there are more than two authors, 'et al.' is used.\n%\n\\institute{University of Mannheim, Mannheim, Germany }\n%\n\\maketitle              % typeset the header of the contribution\n%\n\\begin{abstract}\nThe purpose of the following document is to provide an overview of  \nModel-Based Testing (MBT), highlighting its differences with other testing approaches and especially focusing on its application alongside agile development methodologies. \nThe document will then explain how model-based testing facilitates the automation of test case generation, and how it relates to model-driven development and constraint programming.\nThe advantages and challenges of this testing methodology will be exposed, and a summary and evaluation of tools for its implementation will be provided.\n\n\n\\keywords{Testing  \\and Modeling \\and Model-Based Testing \\and Agile Development}\n\\end{abstract}\n%\n%\n%\n\\section{Introduction}\nWith the increasing adoption of Agile development methodologies and the shortening of the development cycles it has become imperative for software testers to find  better and more efficient ways to evaluate the quality of software products. The automation of test execution has reduced significantly the time required for testing while also increasing the re-usability of the generated test suite, but still it depends on the tester to generate the right test cases that satisfy the quality requirements for the system or module. This has led to the development of approaches that seek to automate the test generation process, the most common of which is known as Model-based testing.\n\\section{Model-Based Testing}\n\n\\subsection{The testing process}\n\nThe most basic task of the software tester is to generate a set of test cases (known also as Test Suite) and then run those tests in the System Under Test (SUT) in order to evaluate the quality of the system against the informal requirements. For the tester to be able to evaluate whether a test passed successfully or not, a Test Oracle is required, which is a list, method or system that provides the correct results or behavior of a system in response to a specific set of inputs. \n\nIdeally the tester will create an exhaustive set of tests that go through every aspect of the program and verify that every possible input produces the required state. In reality though, this is impossible, since the amount of tests could be infinite. Moreover, systems are developed under strict  time and budget deadlines, so only a fractional amount of the possible test cases can be executed before release. It is therefore the job of the tester to determine which set of test cases should be executed, and to provide information on which parts of the overall system have been evaluated, and which parts haven’t. \n\nThe most resource consuming part of the testing process is the execution of the previously generated tests. This is also a boring and repetitive task, and can also be very expensive since it requires a lot of human effort. It is also prone to human errors. To tackle this several test automation tools exist in the market. The goal of test automation is for the tester to use a programming language to define the test once, and then execute these test automatically whenever necessary. This allows for concepts such as Continuous Integration to be possible, since developers can now automatically trigger the regression test suite every time they push a new version of the module they are working on.\n\nHowever, the test automation of the test execution still depends on the tester to design the test suite, which means there is still the need for a test oracle, and there is still the problem of generating the right tests to ensure the desired coverage. Automated test suites also have the disadvantage of not being very flexible, since small changes on the way the system works (A change in the system’s API for example) renders a lot of tests useless.\n\n\\subsection{Model-Based Testing}\n\nNew testing approaches allow us to  take automation one step further. Instead of having the tester generate the test suite by himself, the role of the tester is now to determine an abstract version of the system based on the requirements, and then allowing tools to automatically generate a set of tests from that version. This allows the tester to automatically generate a set of tests that satisfy the desired coverage. This new role also allows the tester to perform an evaluation of the informal requirements, so ambiguous or incomplete requirements can also be quickly found. As a result, the testing process can now begin before a single line of code is written by the developers, and finding errors in the requirement gathering/design phase of the software development life cycle ends up saving a lot of time and money. This is where the Model-based testing comes into play.\n\nModel-Based Testing can therefore be defined as the “automation of the design of black box tests”. It is an application of concepts derived from the more general Model Driven Development (MDD), and can take advantage of most of the same (or similar) modeling languages. \n\n\\subsection{ Modeling for testing}\nEvery system developed has as a starting point a set of informal requirements, provided by the client or one of the shareholders. The client usually has no knowledge of how a system is built, and as such the requirements are usually expressed in a non technical language, in terms of desired results or functionality. Therefore, before a system can be built it is necessary to transform this requirements into something the developers can actually use to begin building the system. Models provide a solution for this process. Software models are abstractions of the required functionalities of a system, taking advantage of visual techniques or an abstract language to represent the intended design.\nThere are several object modeling languages and techniques that can be used to create these models, one of the most common being the Universal Modeling Language (UML). \nAn important objective of modeling is to identify ambiguities in the requirements, as well as defining the scope of the implementation.\n\n\\subsection{ Differences between Modeling for Development and Modeling for testing}\nIt might be tempting to reuse the same models created by the development team for testing purposes. However this is not considered a good practice. Models created for developing tend to have way too much detail, especially regarding low-level implementation issues, which may not be relevant for testing. These models also tend to to not be very specific regarding data processing and manipulation, and focus instead on the structure of the SUT.\n\nThere are therefore two characteristics that a good model should satisfy for test generation: They must be small in comparison to the scope of the whole system, which reduces their cost and complexity, but should be detailed enough that they include all the characteristics relevant to the desired test. It is the job of a good test modeler to find this balance, and it is not very common to find models created for development that satisfy these conditions.\nFinally, using the same models as the development team would mean that any error injected in the design phase or in the requirements will persist, since the tests are going to be applied to the same incorrect model.\n\n\\section{The Model-Based testing process}\nThe process of creating the appropriate tests from a set of requirements consists on the following steps:\n\n\\subsection{Model Definition}\nFirst the level of abstraction should be decided. This is done with the objective of reducing the detail of the model as much as possible while still covering the test objectives. Next the data, operations, outputs and the interactions of the module with other systems or subsystems have to be considered. Unless different input parameters change the behavior of a particular operation, or produce a change in state, it is recommended to leave them out of the model for simplicity. Input parameters are better dealt with once the abstract tests have been generated and being translated into executable tests.\nOutputs of operations should only be included if their value can be used for the test oracle. \nThe behavior of the model can be different than the one of the SUT. Complex operations can be split into smaller steps for a more in depth test of the process. Sequence of operations in the SUT can also be aggregated into a single operation in the model if the result is the only aspect being tested.\n\nNext it is necessary to define the notation language that is going to be used to express the model. These can be classified into two major paradigms: Transition-based models and State-based (also known as pre/post) notations. A brief overview of each is given below.\n\n\\subsubsection{Transition based models}\n With this type of notation the SUT is represented as a series of states and the transitions between them. If determined correctly, each state in the model should result in a change in behavior of the system.Usually these models are expressed using nodes to represent the states and arrows to represent the different transitions. The most simple example of transition based models are finite state machines (FSMs), although there are other languages such as UML State Machines that allow to expand the models with variables, hierarchies or parallelism between machines. Transition based models are recommended for control-oriented systems.\n\n\\subsubsection{Pre/Post Models}\nThis type of notation models the SUT using variables to represent the internal state of the system and the operations that modify them. These operations are defined by preconditions and postconditions. Some of the most used examples of this paradigm are the B notation and UML Object Constraint Language (OCL).Pre/Post models are recommended for data oriented systems.\n\n\nFinally the model should be verified and validated. Verification concerns making sure the model has been created correctly, while validation checks that the model does represent the aspects of the SUT that need to be tested.\n\n\n\\subsection{Generation of abstract test cases}\n\nIt is in this stage that the tester should determine the appropriate selection criteria to generate the necessary tests from the model. \n\n\\subsubsection{Coverage Criteria}\n\nA simple benchmark for test generation can be, for example, to generate tests such as all the requirements have been covered (Requirements-based criteria). Selection criteria can also be based on the coverage criteria usually used in white box testing, but applied in a prescriptive way. The three most relevant types of coverage ate Statement Coverage (SC), Decision Coverage (DC) and Path Coverage (PC).To achieve statement coverage, every single state within the system should be reached at one point by the tests. Decision Coverage, as it name suggests, is concerned with every decision, and to achieve it the tests must follow every option on every decision at least once. Satisfying DC also implies satisfying SC.Finally, the strongest criteria, Path Coverage, requires every single satisfiable path to be traversed by the tests.\n These can be subdivided into structural model criteria, which seek to achieve coverage of the control-flow, and data coverage criteria, which aim to cover the input data space of an operation or a transition.\n\n\\subsubsection{Domain Specific and Statistical Criteria}\nThe tester may also want to apply his previous experiences and his knowledge of the domain space to explicitly specify the set of tests that should be generated, or to focus the tests on a specific kind of fault. Finally, random generation can also be used as a criteria, especially when there is a limited amount of time and a general overview of the behaviors of the system is desired.\n\n\\subsubsection{Selecting the Right Criteria}\nThe amount of tests generated, and the time it will take the tool to do so are influenced on the selected criteria.It is therefore recommended to select different selection criteria depending on the part of the SUT that is being tested. The most commonly used tools for test generation are flexible enough to allow this.  Some examples are LEIRIOS Test Generator (Which generates tests from models that use the B notation), and Qtronic.\nStandards and regulations regarding the minimum of coverage required for some specific domain spaces should also be considered.\n\n\n\n\\subsection{Concretization}\nAt this stage it is assumed that a working version of the part of the SUT that needs to be tested is already created and initialized.\n\nThe output of the test generation stage is a suite of abstract test cases, meaning that they cannot be directly executed in the SUT. This increases the reusability and flexibility of the tests, but creates the necessity of bridging the gap between the abstract model and the SUT. \n\nIn this section, the further work that needs to be done in order to transform an abstract test case into an executable one will be reviewed. This process is known as concretization. \nThere are two main approaches for concretization of a test suite: the adaptation approach and the transformation approach. \n\n\\subsubsection{The Adaptation Approach}\n\nThe adaptation approach involves creating an adapter code that interprets the operations of the model and executes them in the SUT. First, model-level operations and abstract input values are converted into concrete SUT operations. These are then executed in the SUT and the outputs (or any other relevant results) are captured. These are then converted back again into abstract values, which are then fed back into the model. The model’s rules then are used to determine whether the test passed or not. These transformations are done in real time, as the abstract test suite is interpreted. The adaptation approach is recommended when dealing with online testing, where the adapter’s API can provided a tighty integrated two-way connection between the SUT interface and the model.\n\n\\subsection{The Transformation Approach}\n\nThe transformation approach involves creating executable test scripts out of the abstract tests. These can be created in standard programming or test scripting languages. These tests can then be executed and evaluated by the testing team. The transformation approach can be very useful for organizations that already have a mature testing process and therefore have already defined an automated test execution framework. The right tool can then be used to adapt the tests created by the model directly into the existent framework. These generated tests can follow the same organization and version management standards currently defined. In this case the adoption of model-based testing would be less disruptive, since the current test management tools, repositories, test execution platforms, and test reporting processes don’t have to be changed.\nHowever, if the organization’s processes are not mature enough then it is very important to first define an appropriate structure to the test suite, so the hundreds (or even thousands) of generated tests can be tracked. Naming conventions, folders and hierarchies should be applied. Treaceability from the test scripts, the abstract tests and the original requirements must also be maintained.\n\nA third option is to apply a mix of the two approaches. An adapter code can be added to the SUT with the purpose of raising the abstraction level, bringing it closer to the abstraction of the model and making the transformation easier.\n\n\\subsection{Execution and Analysis}\nFinally the tests can be executed and the results can be analyzed. Some model-based testing tools can also handle this phase, or organizations can use the tools they already have for managing the execution and documentation of test scripts.  It is important to note that a failed test can also be caused due to an error in the adapter or in the model itself. During the initial runs of the test suite a lot of feedback about the correctness of the model is obtained, and a higher amount of errors are expected. The testers should then use this feedback to improve the quality of the model, correct adapter errors or even identify flaws in the requirements.\n\n\\section{Advantages of model based testing}\nIn this section the advantages that adopting model-based testing can bring to organizations will be discussed.\n\nReduced cost and time\nModel-based testing can have a significant impact in the testing time if the effort to create and maintain the model is lower than the effort directed to manually create the test suite. There are also several time saving advantages after the tests have been executed, during the analysis and debugging phase. Generated tests report failures in a more consistent way and some tools provide the capability of finding the shortest test sequence that produces a failure, which makes debugging easier. A successful model-based testing implementation will result in the automation of the majority of the testing process, and subsequently results in a decrease in development costs.\n\n\n\n\\subsection{Improved test quality}\n\nManually deriving test cases from requirements is a laborious and error-prone process whose success depends on the ingenuity of the testers. Manually scripted tests are also software programs and are therefore also vulnerable to error injection, meaning that the test scripts themselves have to be tested as well. Model-based testing approaches therefore produce more consistent and reliable tests.\nManually creating a test suite that satisfies the desired coverage can be almost impossible, since even simple programs can involve a very large amount of paths. An industry survey by CA technologies in 2015 found that over 42 percent of the participants “cited a lack of test coverage creating defects and rework as a main software challenge.”\nModel-based testing also produces more optimized tests, and the resulting test suite usually contains the smallest amount of paths needed for maximum coverage. \n\n\n\\subsection{Requirements defect detection}\n\nWriting a model can also expose issues with the requirements. Incomplete, ambiguous or contradictory requirements become more apparent when translated to the precise semantics required by the model. Errors found in the requirements will not be carried over to the design and implementation phases. This is a major benefit since the cost of fixing errors in software products increases exponentially by every phase of the development process. Figure xxx illustrates this increase.\n\n\\subsection{Traceability}\n\nModel-based testing also provides the ability to quickly relate every test to the model and to the requirements.This is called a traceability matrix. If requirements change and the model evolves, traceability becomes very useful for optimization since it allows the tester to focus on the execution of the tests related to the changes.\n\n\n\n\\section{Organizational requirements for Successful Model-Based Testing implementations}\nThe implementation of Model-Based testing practices in an organization will imply a significant change in paradigm in the way the organization approaches Software quality, and will most likely involve additional costs in personnel training, tool acquisitions and process re-engineering. Therefore, in order to increase the probability of a successful MBT implementation some factors should be considered. The first pre-requisite is for the organization to have in place a mature and well defined software development process, with well established channels of communication between the testing team and the rest of the project’s shareholders. \n\n\\subsection{The Test Maturity Model}\n\nIn order to assess the testing maturity level of organizations, the Illinois Institute of Technology developed the Test Maturity Model, which is based on the Capability Maturity Model but focuses on the testing process and the monitoring of quality. It is also divided into five levels of maturity. They are the following:\n\nLevel 1: Initial The organization lacks resources, tools, and trained staff.\n\nLevel 2: Phase Definition: Basic testing methods and techniques are in place.\n\nLevel 3: Integration: Testing is integrated into an entire life cycle, and the roles of tester and developer are separated.\n\nLevel 4: Management and Measurement Testing is a measured and quantified process, defects are logged and given severity levels.\n\nLevel 5: Optimized Testing is continuously improved, the organization invests in tools to support the design of test cases and the administration of defects.\n\nA level 5 Test maturity level is required for a Model-Based testing implementation, since it involves the adoption of new tools and the improvement of the testing process. It is also recommended that the organization has already in place a test execution automation process that can be adapted to work with the models.\n\n\\subsection{The Modeling Maturity Level}\n\nAn organization may also want to considered their current Modeling Maturity Level, especially with the aim of determining correctly the appropriate amount of investment that has to be made on personnel training. It consists on five levels:\n\nLevel 0: No Specification.\n\nLevel 1: Textual:Consists of informal, natural-language documents.\n\nLevel 2: Text with Diagrams: High level diagrams complement the text specifications.\nLevel 3:  Models with Text.\n\nLevel 4:  Precise Models: Recommended level for Model Driven Architecture approaches. The model has a precise meaning, and some code is automatically generated from the model.\n\nLevel 5:  Models Only:the model is used like a high-level programming language.\n\nSince the models created for MBT don’t have to be as detailed as the ones for MDT, a level 3 maturity should be enough for an organization to feel confident in their implementation.\n\n\\section{Limitations of Model-Based Testing}\nModel-Based testing suffers from the same limitation that all testing approaches, since it is impossible to guarantee that all the defects present in the system will be found.This would require all the possible paths, variable combinations and environments to be tested, and that would involve an almost infinite amount of tests.\n\nMBT is also mostly used for functional testing, which means that its implementation will not help with the testing of non-functional requirements. This types of testing include performance, volume, stress, security, compatibility, usability and scalability testing.\n\nSome applications are more suited for MBT than others. Applications whose execution cannot be automated, or that require human interaction are probably not going to benefit as much from an MBT implementation. In these cases the benefits may come from producing a smaller, more efficient test suite.\n\n\n\\section{Model-Based Testing in Agile Development environments}\n\\subsection{Test-Driven Development}\nIn some Agile methods it is a good practice to write the unit tests for a component before the component itself, so the development can be guided since the beginning to satisfy the tests, and so they can be run every time the code is modified or refactored. These tests can also be used to document the code, and make regression testing easier. Model-Based testing can be used to rapidly generate these tests, ensuring a better quality test suite.\n\\subsection{Generation of Acceptance Tests}\n\nIn agile methodologies such as Extreme Programming it is important for the customer on site to write first acceptance tests to drive the goals of the iteration. Model-based testing help to generate better acceptance tests faster. A model can be created from the user stories or an existing model can be evolved with the new functionalities, and the testers then generate from it an adequate acceptance test suite.\nModeling the requirements, as mentioned in previous sections, helps to quickly identify potential errors, inconsistencies or holes in them. \n\n\n\\subsection{Agile Modeling}\n\nAgile Modeling is a methodology that seeks to implement agile values, principles and practices to software modeling. Some of these principles can be applied to the development of modelling for testing. They include the encouragement of model simplicity, iterative approaches, and  a focus on the generated test scripts from the model, instead of creating models just for documentation purposes.\n\n\n\n\n\\section{Survey of tools}\nSurvey of tools\n\nA short overview of 3 Model-Based tools will be provided.\n\\subsection{fMBT}\nfMBT is an open source tool that provides a model editor, test generator, generic adapters for testing through various interfaces, and tools for analyzing logs. It runs on generic Linux platforms, and supports steps written in C++, Python, JavaScript and shell script. fMBT can also be easily integrated into existing regression test suites or continuous integration systems.\n\n\n\\subsection{MaTeLo}\nMaTeLo is a Model-Based testing suite created by All4Tec. Per their website, it offers “a complete solution of Model-Based Testing composed of a powerful environment and a user-friendly interface in order to ease your validation activities.” It provides a graphical test design tool, automated test case and test script generation and requirement traceability. It also facilitates online collaborative work.\n\n\n\n\\subsection{4test}\n4test is a cloud based tool that provides support for DEVOPS. It includes tools to easily generate acceptance criteria based on specifications written in a  Gherkin-based syntax, which is easier for the customer on site and the other stakeholders to understand. It also automates the test design and script generation.\n\n\n\\section{Conclusions}\n\n\n\nFor citations of references, we prefer the use of square brackets\nand consecutive numbers. Citations using labels or the author/year\nconvention are also acceptable. The following bibliography provides\na sample reference list with entries for journal\narticles~\\cite{ref_article1}, an LNCS chapter~\\cite{ref_lncs1}, a\nbook~\\cite{ref_book1}, proceedings without editors~\\cite{ref_proc1},\nand a homepage~\\cite{ref_url1}. Multiple citations are grouped\n\\cite{ref_article1,ref_lncs1,ref_book1},\n\\cite{ref_article1,ref_book1,ref_proc1,ref_url1}.\n%\n% ---- Bibliography ----\n%\n% BibTeX users should specify bibliography style 'splncs04'.\n% References will then be sorted and formatted in the correct style.\n%\n% \\bibliographystyle{splncs04}\n% \\bibliography{mybibliography}\n%\n\\begin{thebibliography}{8}\n\\bibitem{ref_article1}\nAuthor, F.: Article title. Journal \\textbf{2}(5), 99--110 (2016)\n\n\\bibitem{ref_lncs1}\nAuthor, F., Author, S.: Title of a proceedings paper. In: Editor,\nF., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.\nSpringer, Heidelberg (2016). \\doi{10.10007/1234567890}\n\n\\bibitem{ref_book1}\nAuthor, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,\nLocation (1999)\n\n\\bibitem{ref_proc1}\nAuthor, A.-B.: Contribution title. In: 9th International Proceedings\non Proceedings, pp. 1--2. Publisher, Location (2010)\n\n\\bibitem{ref_url1}\nLNCS Homepage, \\url{http://www.springer.com/lncs}. Last accessed 4\nOct 2017\n\\end{thebibliography}\n\\end{document}\n",
			"settings":
			{
				"buffer_size": 28021,
				"line_ending": "Unix",
				"name": "% This is samplepaper.tex, a sample chapter demons"
			}
		},
		{
			"contents": "% This is samplepaper.tex, a sample chapter demonstrating the\n% LLNCS macro package for Springer Computer Science proceedings;\n% Version 2.20 of 2017/10/04\n%\n\\documentclass[runningheads]{llncs}\n%\n\\usepackage{graphicx}\n% Used for displaying a sample figure. If possible, figure files should\n% be included in EPS format.\n%\n% If you use the hyperref package, please uncomment the following line\n% to display URLs in blue roman font according to Springer's eBook style:\n% \\renewcommand\\UrlFont{\\color{blue}\\rmfamily}\n\n\\begin{document}\n%\n\\title{An Overview of Model-Based Testing\\thanks{Supported by organization x.}}\n%\n%\\titlerunning{Abbreviated paper title}\n% If the paper title is too long for the running head, you can set\n% an abbreviated paper title here\n%\n\\author{Jorge Villalobos Rebollosa\\inst{1}\\orcidID{0000-1111-2222-3333}}\n%\n\n\n% First names are abbreviated in the running head.\n% If there are more than two authors, 'et al.' is used.\n%\n\\institute{University of Mannheim, Mannheim, Germany }\n%\n\\maketitle              % typeset the header of the contribution\n%\n\\begin{abstract}\nThe purpose of the following document is to provide an overview of  \nModel-Based Testing (MBT), highlighting its differences with other testing approaches and especially focusing on its application alongside agile development methodologies. \nThe document will then explain how model-based testing facilitates the automation of test case generation, and how it relates to model-driven development and constraint programming.\nThe advantages and challenges of this testing methodology will be exposed, and a summary and evaluation of tools for its implementation will be provided.\n\n\n\\keywords{Testing  \\and Modeling \\and Model-Based Testing \\and Agile Development}\n\\end{abstract}\n%\n%\n%\n\\section{Introduction}\nWith the increasing adoption of Agile development methodologies and the shortening of the development cycles it has become imperative for software testers to find  better and more efficient ways to evaluate the quality of software products. The automation of test execution has reduced significantly the time required for testing while also increasing the re-usability of the generated test suite, but still it depends on the tester to generate the right test cases that satisfy the quality requirements for the system or module. This has led to the development of approaches that seek to automate the test generation process, the most common of which is known as Model-based testing.\n\\section{Model-Based Testing}\n\n\\subsection{The testing process}\n\nThe most basic task of the software tester is to generate a set of test cases (known also as Test Suite) and then run those tests in the System Under Test (SUT) in order to evaluate the quality of the system against the informal requirements. For the tester to be able to evaluate whether a test passed successfully or not, a Test Oracle is required, which is a list, method or system that provides the correct results or behavior of a system in response to a specific set of inputs. \n\nIdeally the tester will create an exhaustive set of tests that go through every aspect of the program and verify that every possible input produces the required state. In reality though, this is impossible, since the amount of tests could be infinite. Moreover, systems are developed under strict  time and budget deadlines, so only a fractional amount of the possible test cases can be executed before release. It is therefore the job of the tester to determine which set of test cases should be executed, and to provide information on which parts of the overall system have been evaluated, and which parts haven’t. \n\nThe most resource consuming part of the testing process is the execution of the previously generated tests. This is also a boring and repetitive task, and can also be very expensive since it requires a lot of human effort. It is also prone to human errors. To tackle this several test automation tools exist in the market. The goal of test automation is for the tester to use a programming language to define the test once, and then execute these test automatically whenever necessary. This allows for concepts such as Continuous Integration to be possible, since developers can now automatically trigger the regression test suite every time they push a new version of the module they are working on.\n\nHowever, the test automation of the test execution still depends on the tester to design the test suite, which means there is still the need for a test oracle, and there is still the problem of generating the right tests to ensure the desired coverage. Automated test suites also have the disadvantage of not being very flexible, since small changes on the way the system works (A change in the system’s API for example) renders a lot of tests useless.\n\n\\subsection{Model-Based Testing}\n\nNew testing approaches allow us to  take automation one step further. Instead of having the tester generate the test suite by himself, the role of the tester is now to determine an abstract version of the system based on the requirements, and then allowing tools to automatically generate a set of tests from that version. This allows the tester to automatically generate a set of tests that satisfy the desired coverage. This new role also allows the tester to perform an evaluation of the informal requirements, so ambiguous or incomplete requirements can also be quickly found. As a result, the testing process can now begin before a single line of code is written by the developers, and finding errors in the requirement gathering/design phase of the software development life cycle ends up saving a lot of time and money. This is where the Model-based testing comes into play.\n\nModel-Based Testing can therefore be defined as the “automation of the design of black box tests”. It is an application of concepts derived from the more general Model Driven Development (MDD), and can take advantage of most of the same (or similar) modeling languages. \n\n\\subsection{ Modeling for testing}\nEvery system developed has as a starting point a set of informal requirements, provided by the client or one of the shareholders. The client usually has no knowledge of how a system is built, and as such the requirements are usually expressed in a non technical language, in terms of desired results or functionality. Therefore, before a system can be built it is necessary to transform this requirements into something the developers can actually use to begin building the system. Models provide a solution for this process. Software models are abstractions of the required functionality of a system, taking advantage of visual techniques or an abstract language to represent the intended design.\nThere are several object modeling languages and techniques that can be used to create these models, one of the most common being the Universal Modeling Language (UML). \nAn important objective of modeling is to identify ambiguities in the requirements, as well as defining the scope of the implementation.\n\n\\subsection{ Differences between Modeling for Development and Modeling for testing}\nIt might be tempting to reuse the same models created by the development team for testing purposes. However this is not considered a good practice. Models created for developing tend to have way too much detail, especially regarding low-level implementation issues, which may not be relevant for testing. These models also tend to to not be very specific regarding data processing and manipulation, and focus instead on the structure of the SUT.\n\nThere are therefore two characteristics that a good model should satisfy for test generation: They must be small in comparison to the scope of the whole system, which reduces their cost and complexity, but should be detailed enough that they include all the characteristics relevant to the desired test. It is the job of a good test modeler to find this balance, and it is not very common to find models created for development that satisfy these conditions.\nFinally, using the same models as the development team would mean that any error injected in the design phase or in the requirements will persist, since the tests are going to be applied to the same incorrect model.\n\n\\section{The Model-Based testing process}\nThe process of creating the appropriate tests from a set of requirements consists on the following steps:\n\n\\subsection{Model Definition}\nFirst the level of abstraction should be decided. This is done with the objective of reducing the detail of the model as much as possible while still covering the test objectives. Next the data, operations, outputs and the interactions of the module with other systems or subsystems have to be considered. Unless different input parameters change the behavior of a particular operation, or produce a change in state, it is recommended to leave them out of the model for simplicity. Input parameters are better dealt with once the abstract tests have been generated and being translated into executable tests.\nOutputs of operations should only be included if their value can be used for the test oracle. \nThe behavior of the model can be different than the one of the SUT. Complex operations can be split into smaller steps for a more in depth test of the process. Sequence of operations in the SUT can also be aggregated into a single operation in the model if the result is the only aspect being tested.\n\nNext it is necessary to define the notation language that is going to be used to express the model. These can be classified into two major paradigms: Transition-based models and State-based (also known as pre/post) notations. A brief overview of each is given below.\n\n\\subsubsection{Transition based models}\n With this type of notation the SUT is represented as a series of states and the transitions between them. If determined correctly, each state in the model should result in a change in behavior of the system.Usually these models are expressed using nodes to represent the states and arrows to represent the different transitions. The most simple example of transition based models are finite state machines (FSMs), although there are other languages such as UML State Machines that allow to expand the models with variables, hierarchies or parallelism between machines. Transition based models are recommended for control-oriented systems.\n\n\\subsubsection{Pre/Post Models}\nThis type of notation models the SUT using variables to represent the internal state of the system and the operations that modify them. These operations are defined by preconditions and postconditions. Some of the most used examples of this paradigm are the B notation and UML Object Constraint Language (OCL).Pre/Post models are recommended for data oriented systems.\n\n\nFinally the model should be verified and validated. Verification concerns making sure the model has been created correctly, while validation checks that the model does represent the aspects of the SUT that need to be tested.\n\n\n\\subsection{Generation of abstract test cases}\n\nIt is in this stage that the tester should determine the appropriate selection criteria to generate the necessary tests from the model. \n\n\\subsubsection{Coverage Criteria}\n\nA simple benchmark for test generation can be, for example, to generate tests such as all the requirements have been covered (Requirements-based criteria). Selection criteria can also be based on the coverage criteria usually used in white box testing, but applied in a prescriptive way. The three most relevant types of coverage ate Statement Coverage (SC), Decision Coverage (DC) and Path Coverage (PC).To achieve statement coverage, every single state within the system should be reached at one point by the tests. Decision Coverage, as it name suggests, is concerned with every decision, and to achieve it the tests must follow every option on every decision at least once. Satisfying DC also implies satisfying SC.Finally, the strongest criteria, Path Coverage, requires every single satisfiable path to be traversed by the tests.\n These can be subdivided into structural model criteria, which seek to achieve coverage of the control-flow, and data coverage criteria, which aim to cover the input data space of an operation or a transition.\n\n\\subsubsection{Domain Specific and Statistical Criteria}\nThe tester may also want to apply his previous experiences and his knowledge of the domain space to explicitly specify the set of tests that should be generated, or to focus the tests on a specific kind of fault. Finally, random generation can also be used as a criteria, especially when there is a limited amount of time and a general overview of the behaviors of the system is desired.\n\n\\subsubsection{Selecting the Right Criteria}\nThe amount of tests generated, and the time it will take the tool to do so are influenced on the selected criteria.It is therefore recommended to select different selection criteria depending on the part of the SUT that is being tested. The most commonly used tools for test generation are flexible enough to allow this.  Some examples are LEIRIOS Test Generator (Which generates tests from models that use the B notation), and Qtronic.\nStandards and regulations regarding the minimum of coverage required for some specific domain spaces should also be considered.\n\n\n\n\\subsection{Concretization}\nAt this stage it is assumed that a working version of the part of the SUT that needs to be tested is already created and initialized.\n\nThe output of the test generation stage is a suite of abstract test cases, meaning that they cannot be directly executed in the SUT. This increases the reusability and flexibility of the tests, but creates the necessity of bridging the gap between the abstract model and the SUT. \n\nIn this section, the further work that needs to be done in order to transform an abstract test case into an executable one will be reviewed. This process is known as concretization. \nThere are two main approaches for concretization of a test suite: the adaptation approach and the transformation approach. \n\n\\subsubsection{The Adaptation Approach}\n\nThe adaptation approach involves creating an adapter code that interprets the operations of the model and executes them in the SUT. First, model-level operations and abstract input values are converted into concrete SUT operations. These are then executed in the SUT and the outputs (or any other relevant results) are captured. These are then converted back again into abstract values, which are then fed back into the model. The model’s rules then are used to determine whether the test passed or not. These transformations are done in real time, as the abstract test suite is interpreted. The adaptation approach is recommended when dealing with online testing, where the adapter’s API can provided a tighty integrated two-way connection between the SUT interface and the model.\n\n\\subsection{The Transformation Approach}\n\nThe transformation approach involves creating executable test scripts out of the abstract tests. These can be created in standard programming or test scripting languages. These tests can then be executed and evaluated by the testing team. The transformation approach can be very useful for organizations that already have a mature testing process and therefore have already defined an automated test execution framework. The right tool can then be used to adapt the tests created by the model directly into the existent framework. These generated tests can follow the same organization and version management standards currently defined. In this case the adoption of model-based testing would be less disruptive, since the current test management tools, repositories, test execution platforms, and test reporting processes don’t have to be changed.\nHowever, if the organization’s processes are not mature enough then it is very important to first define an appropriate structure to the test suite, so the hundreds (or even thousands) of generated tests can be tracked. Naming conventions, folders and hierarchies should be applied. Treaceability from the test scripts, the abstract tests and the original requirements must also be maintained.\n\nA third option is to apply a mix of the two approaches. An adapter code can be added to the SUT with the purpose of raising the abstraction level, bringing it closer to the abstraction of the model and making the transformation easier.\n\n\\subsection{Execution and Analysis}\nFinally the tests can be executed and the results can be analyzed. Some model-based testing tools can also handle this phase, or organizations can use the tools they already have for managing the execution and documentation of test scripts.  It is important to note that a failed test can also be caused due to an error in the adapter or in the model itself. During the initial runs of the test suite a lot of feedback about the correctness of the model is obtained, and a higher amount of errors are expected. The testers should then use this feedback to improve the quality of the model, correct adapter errors or even identify flaws in the requirements.\n\n\\section{Advantages of model based testing}\nIn this section the advantages that adopting model-based testing can bring to organizations will be discussed.\n\nReduced cost and time\nModel-based testing can have a significant impact in the testing time if the effort to create and maintain the model is lower than the effort directed to manually create the test suite. There are also several time saving advantages after the tests have been executed, during the analysis and debugging phase. Generated tests report failures in a more consistent way and some tools provide the capability of finding the shortest test sequence that produces a failure, which makes debugging easier. A successful model-based testing implementation will result in the automation of the majority of the testing process, and subsequently results in a decrease in development costs.\n\n\n\n\\subsection{Improved test quality}\n\nManually deriving test cases from requirements is a laborious and error-prone process whose success depends on the ingenuity of the testers. Manually scripted tests are also software programs and are therefore also vulnerable to error injection, meaning that the test scripts themselves have to be tested as well. Model-based testing approaches therefore produce more consistent and reliable tests.\nManually creating a test suite that satisfies the desired coverage can be almost impossible, since even simple programs can involve a very large amount of paths. An industry survey by CA technologies in 2015 found that over 42 percent of the participants “cited a lack of test coverage creating defects and rework as a main software challenge.”\nModel-based testing also produces more optimized tests, and the resulting test suite usually contains the smallest amount of paths needed for maximum coverage. \n\n\n\\subsection{Requirements defect detection}\n\nWriting a model can also expose issues with the requirements. Incomplete, ambiguous or contradictory requirements become more apparent when translated to the precise semantics required by the model. Errors found in the requirements will not be carried over to the design and implementation phases. This is a major benefit since the cost of fixing errors in software products increases exponentially by every phase of the development process. Figure xxx illustrates this increase.\n\n\\subsection{Traceability}\n\nModel-based testing also provides the ability to quickly relate every test to the model and to the requirements.This is called a traceability matrix. If requirements change and the model evolves, traceability becomes very useful for optimization since it allows the tester to focus on the execution of the tests related to the changes.\n\n\n\n\\section{Organizational requirements for Successful Model-Based Testing implementations}\nThe implementation of Model-Based testing practices in an organization will imply a significant change in paradigm in the way the organization approaches Software quality, and will most likely involve additional costs in personnel training, tool acquisitions and process re-engineering. Therefore, in order to increase the probability of a successful MBT implementation some factors should be considered. The first pre-requisite is for the organization to have in place a mature and well defined software development process, with well established channels of communication between the testing team and the rest of the project’s shareholders. \n\n\\subsection{The Test Maturity Model}\n\nIn order to assess the testing maturity level of organizations, the Illinois Institute of Technology developed the Test Maturity Model, which is based on the Capability Maturity Model but focuses on the testing process and the monitoring of quality. It is also divided into five levels of maturity. They are the following:\n\nLevel 1: Initial The organization lacks resources, tools, and trained staff.\n\nLevel 2: Phase Definition: Basic testing methods and techniques are in place.\n\nLevel 3: Integration: Testing is integrated into an entire life cycle, and the roles of tester and developer are separated.\n\nLevel 4: Management and Measurement Testing is a measured and quantified process, defects are logged and given severity levels.\n\nLevel 5: Optimized Testing is continuously improved, the organization invests in tools to support the design of test cases and the administration of defects.\n\nA level 5 Test maturity level is required for a Model-Based testing implementation, since it involves the adoption of new tools and the improvement of the testing process. It is also recommended that the organization has already in place a test execution automation process that can be adapted to work with the models.\n\n\\subsection{The Modeling Maturity Level}\n\nAn organization may also want to considered their current Modeling Maturity Level, especially with the aim of determining correctly the appropriate amount of investment that has to be made on personnel training. It consists on five levels:\n\nLevel 0: No Specification.\n\nLevel 1: Textual:Consists of informal, natural-language documents.\n\nLevel 2: Text with Diagrams: High level diagrams complement the text specifications.\nLevel 3:  Models with Text.\n\nLevel 4:  Precise Models: Recommended level for Model Driven Architecture approaches. The model has a precise meaning, and some code is automatically generated from the model.\n\nLevel 5:  Models Only:the model is used like a high-level programming language.\n\nSince the models created for MBT don’t have to be as detailed as the ones for MDT, a level 3 maturity should be enough for an organization to feel confident in their implementation.\n\n\\section{Limitations of Model-Based Testing}\nModel-Based testing suffers from the same limitation that all testing approaches, since it is impossible to guarantee that all the defects present in the system will be found.This would require all the possible paths, variable combinations and environments to be tested, and that would involve an almost infinite amount of tests.\n\nMBT is also mostly used for functional testing, which means that its implementation will not help with the testing of non-functional requirements. This types of testing include performance, volume, stress, security, compatibility, usability and scalability testing.\n\nSome applications are more suited for MBT than others. Applications whose execution cannot be automated, or that require human interaction are probably not going to benefit as much from an MBT implementation. In these cases the benefits may come from producing a smaller, more efficient test suite.\n\n\n\\section{Model-Based Testing in Agile Development environments}\n\\subsection{Test-Driven Development}\nIn some Agile methods it is a good practice to write the unit tests for a component before the component itself, so the development can be guided since the beginning to satisfy the tests, and so they can be run every time the code is modified or refactored. These tests can also be used to document the code, and make regression testing easier. Model-Based testing can be used to rapidly generate these tests, ensuring a better quality test suite.\n\\subsection{Generation of Acceptance Tests}\n\nIn agile methodologies such as Extreme Programming it is important for the customer on site to write first acceptance tests to drive the goals of the iteration. Model-based testing help to generate better acceptance tests faster. A model can be created from the user stories or an existing model can be evolved with the new functionalities, and the testers then generate from it an adequate acceptance test suite.\nModeling the requirements, as mentioned in previous sections, helps to quickly identify potential errors, inconsistencies or holes in them. \n\n\n\\subsection{Agile Modeling}\n\nAgile Modeling is a methodology that seeks to implement agile values, principles and practices to software modeling. Some of these principles can be applied to the development of modelling for testing. They include the encouragement of model simplicity, iterative approaches, and  a focus on the generated test scripts from the model, instead of creating models just for documentation purposes.\n\n\n\n\n\\section{Survey of tools}\nSurvey of tools\n\nA short overview of 3 Model-Based tools will be provided.\n\\subsection{fMBT}\nfMBT is an open source tool that provides a model editor, test generator, generic adapters for testing through various interfaces, and tools for analyzing logs. It runs on generic Linux platforms, and supports steps written in C++, Python, JavaScript and shell script. fMBT can also be easily integrated into existing regression test suites or continuous integration systems.\n\n\n\\subsection{MaTeLo}\nMaTeLo is a Model-Based testing suite created by All4Tec. Per their website, it offers “a complete solution of Model-Based Testing composed of a powerful environment and a user-friendly interface in order to ease your validation activities.” It provides a graphical test design tool, automated test case and test script generation and requirement traceability. It also facilitates online collaborative work.\n\n\n\n\\subsection{4test}\n4test is a cloud based tool that provides support for DEVOPS. It includes tools to easily generate acceptance criteria based on specifications written in a  Gherkin-based syntax, which is easier for the customer on site and the other stakeholders to understand. It also automates the test design and script generation.\n\n\\section{Successful implementation: MBT application in NASA’s OSAL}\n\nChristoph Schulze, Dharmalingam Ganesan and Mikael Lindvall Fraunhofer of the Center for Experimental Software Engineering College Park presented a case study that evaluated model-based testing in NASA’s Operating System Abstraction Layer (OSAL with the objective of evaluating its applicability and effectiveness. The models were developed using FSMs and from there they generated tests automatically. After the test execution a few previously unknown defects were found.\n\n\n\\section{Conclusions}\nModel-Based testing allows organizations to take test automation to a new level by automating the generation of the black box tests as well. This changes the paradigm of the tester’s role, increasing their involvement in the software design stages and introducing testing in a much earlier stage.There are many benefits that organizations can get from pursuing a Model-Based testing implementation, including the creation of a better, streamlined test suite that pre-satisfies the coverage criteria determined by the requirements and the automatic generation, execution and administration of test scripts and test results.\nHowever, before going through with such an implementation, the interested organizations must consider their own test and modeling maturity levels, to avoid introducing too much disruption to their software development process.\nModel-Based testing can be useful in agile development by providing a practical way to generate high quality acceptance tests.\n\n\\section{Future Work and limitations}\nThis document provided a very general overview of the Model-Based Testing process, but anyone interested in the implementation of such a process would require to make a more in depth analysis into the model creation process. The different symbols used in UML, their relationships and correct use, and correct syntax of the notation languages such as OCL and B should be explored further.\n\nFor citations of references, we prefer the use of square brackets\nand consecutive numbers. Citations using labels or the author/year\nconvention are also acceptable. The following bibliography provides\na sample reference list with entries for journal\narticles~\\cite{ref_article1}, an LNCS chapter~\\cite{ref_lncs1}, a\nbook~\\cite{ref_book1}, proceedings without editors~\\cite{ref_proc1},\nand a homepage~\\cite{ref_url1}. Multiple citations are grouped\n\\cite{ref_article1,ref_lncs1,ref_book1},\n\\cite{ref_article1,ref_book1,ref_proc1,ref_url1}.\n%\n% ---- Bibliography ----\n%\n% BibTeX users should specify bibliography style 'splncs04'.\n% References will then be sorted and formatted in the correct style.\n%\n% \\bibliographystyle{splncs04}\n% \\bibliography{mybibliography}\n%\n\\begin{thebibliography}{8}\n\\bibitem{ref_article1}\nAuthor, F.: Article title. Journal \\textbf{2}(5), 99--110 (2016)\n\n\\bibitem{ref_lncs1}\nAuthor, F., Author, S.: Title of a proceedings paper. In: Editor,\nF., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.\nSpringer, Heidelberg (2016). \\doi{10.10007/1234567890}\n\n\\bibitem{ref_book1}\nAuthor, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,\nLocation (1999)\n\n\\bibitem{ref_proc1}\nAuthor, A.-B.: Contribution title. In: 9th International Proceedings\non Proceedings, pp. 1--2. Publisher, Location (2010)\n\n\\bibitem{ref_url1}\nLNCS Homepage, \\url{http://www.springer.com/lncs}. Last accessed 4\nOct 2017\n\\end{thebibliography}\n\\end{document}\n",
			"settings":
			{
				"buffer_size": 29964,
				"line_ending": "Unix",
				"name": "% This is samplepaper.tex, a sample chapter demons"
			}
		},
		{
			"contents": "\nFor citations of references, we prefer the use of square brackets\nand consecutive numbers. Citations using labels or the author/year\nconvention are also acceptable. The following bibliography provides\na sample reference list with entries for journal\narticles~\\cite{ref_article1}, an LNCS chapter~\\cite{ref_lncs1}, a\nbook~\\cite{ref_book1}, proceedings without editors~\\cite{ref_proc1},\nand a homepage~\\cite{ref_url1}. Multiple citations are grouped\n\\cite{ref_article1,ref_lncs1,ref_book1},\n\\cite{ref_article1,ref_book1,ref_proc1,ref_url1}.\n%\n% ---- Bibliography ----\n%\n% BibTeX users should specify bibliography style 'splncs04'.\n% References will then be sorted and formatted in the correct style.\n%\n% \\bibliographystyle{splncs04}\n% \\bibliography{mybibliography}\n%\n\\begin{thebibliography}{8}\n\\bibitem{ref_article1}\nAuthor, F.: Article title. Journal \\textbf{2}(5), 99--110 (2016)\n\n\\bibitem{ref_lncs1}\nAuthor, F., Author, S.: Title of a proceedings paper. In: Editor,\nF., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.\nSpringer, Heidelberg (2016). \\doi{10.10007/1234567890}\n\n\\bibitem{ref_book1}\nAuthor, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,\nLocation (1999)\n\n\\bibitem{ref_proc1}\nAuthor, A.-B.: Contribution title. In: 9th International Proceedings\non Proceedings, pp. 1--2. Publisher, Location (2010)\n\n\\bibitem{ref_url1}\nLNCS Homepage, \\url{http://www.springer.com/lncs}. Last accessed 4\nOct 2017\n\\end{thebibliography}",
			"settings":
			{
				"buffer_size": 1455,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "% This is samplepaper.tex, a sample chapter demonstrating the\n% LLNCS macro package for Springer Computer Science proceedings;\n% Version 2.20 of 2017/10/04\n%\n\\documentclass[runningheads]{llncs}\n%\n\\usepackage{graphicx}\n% Used for displaying a sample figure. If possible, figure files should\n% be included in EPS format.\n%\n% If you use the hyperref package, please uncomment the following line\n% to display URLs in blue roman font according to Springer's eBook style:\n% \\renewcommand\\UrlFont{\\color{blue}\\rmfamily}\n\n\\begin{document}\n%\n\\title{An Overview of Model-Based Testing}\n%\n%\\titlerunning{Abbreviated paper title}\n% If the paper title is too long for the running head, you can set\n% an abbreviated paper title here\n%\n\\author{Jorge Villalobos Rebollosa\\orcidID{1622886}}\n%\n\n\n% First names are abbreviated in the running head.\n% If there are more than two authors, 'et al.' is used.\n%\n\\institute{Chair of Software Engineering, University of Mannheim, Mannheim, Germany }\n%\n\\maketitle              % typeset the header of the contribution\n%\n\\begin{abstract}\nThe purpose of this document is to provide an overview of Model-Based testing, an approach that seeks to automate the generation of black box tests. The traditional testing approaches will be explained and contrasted with model-based testing. The model-based testing process will be explained in four  phases: Model creation, abstract case generation, concretization and execution and evaluation. \n\nThe organizational requirements for a model-based testing implementation will also be explored. THere will be an analysis on the ways in which model-based testing can support agile development methodologies. And finally, a survey of popular model-based testing tools will be provided, and case studies of successful model-based testing implementations will be discussed.\n\n\\keywords{Testing  \\and Modeling \\and Model-Based Testing \\and Agile Development}\n\\end{abstract}\n%\n%\n%\n\\section{Introduction}\nWith the increasing adoption of Agile development methodologies and the shortening of the development cycles it has become imperative for software testers to find  better and more efficient ways to evaluate the quality of software products. The automation of test execution has reduced significantly the time required for testing while also increasing the re-usability of the generated test suite, but still it depends on the tester to generate the right test cases that satisfy the quality requirements for the system or module. This has led to the development of approaches that seek to automate the test generation process, the most common of which is known as Model-based testing.\n\\section{Model-Based Testing}\n\n\\subsection{The testing process}\n\nThe most basic task of the software tester is to generate a set of test cases (known also as Test Suite) and then run those tests in the System Under Test (SUT) in order to evaluate the quality of the system against the informal requirements. For the tester to be able to evaluate whether a test passed successfully or not, a Test Oracle is required, which is a list, method or system that provides the correct results or behavior of a system in response to a specific set of inputs. \n\nIdeally the tester will create an exhaustive set of tests that go through every aspect of the program and verify that every possible input produces the required state. In reality though, this is impossible, since the amount of tests could be infinite. Moreover, systems are developed under strict  time and budget deadlines, so only a fractional amount of the possible test cases can be executed before release. It is therefore the job of the tester to determine which set of test cases should be executed, and to provide information on which parts of the overall system have been evaluated, and which parts haven’t. \n\nThe most resource consuming part of the testing process is the execution of the previously generated tests. This is also a boring and repetitive task, and can also be very expensive since it requires a lot of human effort. It is also prone to human errors. To tackle this several test automation tools exist in the market. The goal of test automation is for the tester to use a programming language to define the test once, and then execute these test automatically whenever necessary. This allows for concepts such as Continuous Integration to be possible, since developers can now automatically trigger the regression test suite every time they push a new version of the module they are working on.\n\nHowever, the test automation of the test execution still depends on the tester to design the test suite, which means there is still the need for a test oracle, and there is still the problem of generating the right tests to ensure the desired coverage. Automated test suites also have the disadvantage of not being very flexible, since small changes on the way the system works (A change in the system’s API for example) renders a lot of tests useless.\n\n\\subsection{Model-Based Testing}\n\nNew testing approaches allow us to  take automation one step further. Instead of having the tester generate the test suite by himself, the role of the tester is now to determine an abstract version of the system based on the requirements, and then allowing tools to automatically generate a set of tests from that version. This allows the tester to automatically generate a set of tests that satisfy the desired coverage. This new role also allows the tester to perform an evaluation of the informal requirements, so ambiguous or incomplete requirements can also be quickly found. As a result, the testing process can now begin before a single line of code is written by the developers, and finding errors in the requirement gathering/design phase of the software development life cycle ends up saving a lot of time and money. This is where the Model-based testing comes into play.\n\nModel-Based Testing can therefore be defined as the “automation of the design of black box tests”. It is an application of concepts derived from the more general Model Driven Development (MDD), and can take advantage of most of the same (or similar) modeling languages. \n\n\\subsection{ Modeling for testing}\nEvery system developed has as a starting point a set of informal requirements, provided by the client or one of the shareholders. The client usually has no knowledge of how a system is built, and as such the requirements are usually expressed in a non technical language, in terms of desired results or functionality. Therefore, before a system can be built it is necessary to transform this requirements into something the developers can actually use to begin building the system. Models provide a solution for this process. Software models are abstractions of the required functionality of a system, taking advantage of visual techniques or an abstract language to represent the intended design.\n\nThere are several object modeling languages and techniques that can be used to create these models, one of the most common being the Universal Modeling Language (UML). \nAn important objective of modeling is to identify ambiguities in the requirements, as well as defining the scope of the implementation.\n\n\\subsection{ Differences between Modeling for Development and Modeling for testing}\nIt might be tempting to reuse the same models created by the development team for testing purposes. However this is not considered a good practice. Models created for developing tend to have way too much detail, especially regarding low-level implementation issues, which may not be relevant for testing. These models also tend to to not be very specific regarding data processing and manipulation, and focus instead on the structure of the SUT.\n\nThere are therefore two characteristics that a good model should satisfy for test generation: They must be small in comparison to the scope of the whole system, which reduces their cost and complexity, but should be detailed enough that they include all the characteristics relevant to the desired test. It is the job of a good test modeler to find this balance, and it is not very common to find models created for development that satisfy these conditions.\nFinally, using the same models as the development team would mean that any error injected in the design phase or in the requirements will persist, since the tests are going to be applied to the same incorrect model.\n\n\\section{The Model-Based testing process}\nThe process of creating the appropriate tests from a set of requirements consists on the following steps:\n\n\\subsection{Model Definition}\nFirst the level of abstraction should be decided. This is done with the objective of reducing the detail of the model as much as possible while still covering the test objectives. Next the data, operations, outputs and the interactions of the module with other systems or subsystems have to be considered. Unless different input parameters change the behavior of a particular operation, or produce a change in state, it is recommended to leave them out of the model for simplicity. Input parameters are better dealt with once the abstract tests have been generated and being translated into executable tests.\nOutputs of operations should only be included if their value can be used for the test oracle. \nThe behavior of the model can be different than the one of the SUT. Complex operations can be split into smaller steps for a more in depth test of the process. Sequence of operations in the SUT can also be aggregated into a single operation in the model if the result is the only aspect being tested.\n\nNext it is necessary to define the notation language that is going to be used to express the model. These can be classified into two major paradigms: Transition-based models and State-based (also known as pre/post) notations. A brief overview of each is given below.\n\n\\subsubsection{Transition based models}\n With this type of notation the SUT is represented as a series of states and the transitions between them. If determined correctly, each state in the model should result in a change in behavior of the system.Usually these models are expressed using nodes to represent the states and arrows to represent the different transitions. The most simple example of transition based models are finite state machines (FSMs), although there are other languages such as UML State Machines that allow to expand the models with variables, hierarchies or parallelism between machines. Transition based models are recommended for control-oriented systems.\n\n\\subsubsection{Pre/Post Models}\nThis type of notation models the SUT using variables to represent the internal state of the system and the operations that modify them. These operations are defined by preconditions and postconditions. Some of the most used examples of this paradigm are the B notation and UML Object Constraint Language (OCL).Pre/Post models are recommended for data oriented systems.\n\n\nFinally the model should be verified and validated. Verification concerns making sure the model has been created correctly, while validation checks that the model does represent the aspects of the SUT that need to be tested.\n\n\n\\subsection{Generation of abstract test cases}\n\nIt is in this stage that the tester should determine the appropriate selection criteria to generate the necessary tests from the model. \n\n\\subsubsection{Coverage Criteria}\n\nA simple benchmark for test generation can be, for example, to generate tests such as all the requirements have been covered (Requirements-based criteria). Selection criteria can also be based on the coverage criteria usually used in white box testing, but applied in a prescriptive way. The three most relevant types of coverage ate Statement Coverage (SC), Decision Coverage (DC) and Path Coverage (PC).To achieve statement coverage, every single state within the system should be reached at one point by the tests. Decision Coverage, as it name suggests, is concerned with every decision, and to achieve it the tests must follow every option on every decision at least once. Satisfying DC also implies satisfying SC.Finally, the strongest criteria, Path Coverage, requires every single satisfiable path to be traversed by the tests.\n These can be subdivided into structural model criteria, which seek to achieve coverage of the control-flow, and data coverage criteria, which aim to cover the input data space of an operation or a transition.\n\n\\subsubsection{Domain Specific and Statistical Criteria}\nThe tester may also want to apply his previous experiences and his knowledge of the domain space to explicitly specify the set of tests that should be generated, or to focus the tests on a specific kind of fault. Finally, random generation can also be used as a criteria, especially when there is a limited amount of time and a general overview of the behaviors of the system is desired.\n\n\\subsubsection{Selecting the Right Criteria}\nThe amount of tests generated, and the time it will take the tool to do so are influenced on the selected criteria.It is therefore recommended to select different selection criteria depending on the part of the SUT that is being tested. The most commonly used tools for test generation are flexible enough to allow this.  Some examples are LEIRIOS Test Generator (Which generates tests from models that use the B notation), and Qtronic.\nStandards and regulations regarding the minimum of coverage required for some specific domain spaces should also be considered.\n\n\n\n\\subsection{Concretization}\nAt this stage it is assumed that a working version of the part of the SUT that needs to be tested is already created and initialized.\n\nThe output of the test generation stage is a suite of abstract test cases, meaning that they cannot be directly executed in the SUT. This increases the reusability and flexibility of the tests, but creates the necessity of bridging the gap between the abstract model and the SUT. \n\nIn this section, the further work that needs to be done in order to transform an abstract test case into an executable one will be reviewed. This process is known as concretization. \nThere are two main approaches for concretization of a test suite: the adaptation approach and the transformation approach. \n\n\\subsubsection{The Adaptation Approach}\n\nThe adaptation approach involves creating an adapter code that interprets the operations of the model and executes them in the SUT. First, model-level operations and abstract input values are converted into concrete SUT operations. These are then executed in the SUT and the outputs (or any other relevant results) are captured. These are then converted back again into abstract values, which are then fed back into the model. The model’s rules then are used to determine whether the test passed or not. These transformations are done in real time, as the abstract test suite is interpreted. The adaptation approach is recommended when dealing with online testing, where the adapter’s API can provided a tighty integrated two-way connection between the SUT interface and the model.\n\n\\subsection{The Transformation Approach}\n\nThe transformation approach involves creating executable test scripts out of the abstract tests. These can be created in standard programming or test scripting languages. These tests can then be executed and evaluated by the testing team. The transformation approach can be very useful for organizations that already have a mature testing process and therefore have already defined an automated test execution framework. The right tool can then be used to adapt the tests created by the model directly into the existent framework. These generated tests can follow the same organization and version management standards currently defined. In this case the adoption of model-based testing would be less disruptive, since the current test management tools, repositories, test execution platforms, and test reporting processes don’t have to be changed.\nHowever, if the organization’s processes are not mature enough then it is very important to first define an appropriate structure to the test suite, so the hundreds (or even thousands) of generated tests can be tracked. Naming conventions, folders and hierarchies should be applied. Treaceability from the test scripts, the abstract tests and the original requirements must also be maintained.\n\nA third option is to apply a mix of the two approaches. An adapter code can be added to the SUT with the purpose of raising the abstraction level, bringing it closer to the abstraction of the model and making the transformation easier.\n\n\\subsection{Execution and Analysis}\nFinally the tests can be executed and the results can be analyzed. Some model-based testing tools can also handle this phase, or organizations can use the tools they already have for managing the execution and documentation of test scripts.  It is important to note that a failed test can also be caused due to an error in the adapter or in the model itself. During the initial runs of the test suite a lot of feedback about the correctness of the model is obtained, and a higher amount of errors are expected. The testers should then use this feedback to improve the quality of the model, correct adapter errors or even identify flaws in the requirements.\n\n\\section{Advantages of model based testing}\nIn this section the advantages that adopting model-based testing can bring to organizations will be discussed.\n\nReduced cost and time\nModel-based testing can have a significant impact in the testing time if the effort to create and maintain the model is lower than the effort directed to manually create the test suite. There are also several time saving advantages after the tests have been executed, during the analysis and debugging phase. Generated tests report failures in a more consistent way and some tools provide the capability of finding the shortest test sequence that produces a failure, which makes debugging easier. A successful model-based testing implementation will result in the automation of the majority of the testing process, and subsequently results in a decrease in development costs.\n\n\n\n\\subsection{Improved test quality}\n\nManually deriving test cases from requirements is a laborious and error-prone process whose success depends on the ingenuity of the testers. Manually scripted tests are also software programs and are therefore also vulnerable to error injection, meaning that the test scripts themselves have to be tested as well. Model-based testing approaches therefore produce more consistent and reliable tests.\nManually creating a test suite that satisfies the desired coverage can be almost impossible, since even simple programs can involve a very large amount of paths. An industry survey by CA technologies in 2015 found that over 42 percent of the participants “cited a lack of test coverage creating defects and rework as a main software challenge.”\nModel-based testing also produces more optimized tests, and the resulting test suite usually contains the smallest amount of paths needed for maximum coverage. \n\n\n\\subsection{Requirements defect detection}\n\nWriting a model can also expose issues with the requirements. Incomplete, ambiguous or contradictory requirements become more apparent when translated to the precise semantics required by the model. Errors found in the requirements will not be carried over to the design and implementation phases. This is a major benefit since the cost of fixing errors in software products increases exponentially by every phase of the development process. Figure xxx illustrates this increase.\n\n\\subsection{Traceability}\n\nModel-based testing also provides the ability to quickly relate every test to the model and to the requirements.This is called a traceability matrix. If requirements change and the model evolves, traceability becomes very useful for optimization since it allows the tester to focus on the execution of the tests related to the changes.\n\n\n\n\\section{Organizational requirements for Successful Model-Based Testing implementations}\nThe implementation of Model-Based testing practices in an organization will imply a significant change in paradigm in the way the organization approaches Software quality, and will most likely involve additional costs in personnel training, tool acquisitions and process re-engineering. Therefore, in order to increase the probability of a successful MBT implementation some factors should be considered. The first pre-requisite is for the organization to have in place a mature and well defined software development process, with well established channels of communication between the testing team and the rest of the project’s shareholders. \n\n\\subsection{The Test Maturity Model}\n\nIn order to assess the testing maturity level of organizations, the Illinois Institute of Technology developed the Test Maturity Model, which is based on the Capability Maturity Model but focuses on the testing process and the monitoring of quality. It is also divided into five levels of maturity. They are the following:\n\nLevel 1: Initial The organization lacks resources, tools, and trained staff.\n\nLevel 2: Phase Definition: Basic testing methods and techniques are in place.\n\nLevel 3: Integration: Testing is integrated into an entire life cycle, and the roles of tester and developer are separated.\n\nLevel 4: Management and Measurement Testing is a measured and quantified process, defects are logged and given severity levels.\n\nLevel 5: Optimized Testing is continuously improved, the organization invests in tools to support the design of test cases and the administration of defects.\n\nA level 5 Test maturity level is required for a Model-Based testing implementation, since it involves the adoption of new tools and the improvement of the testing process. It is also recommended that the organization has already in place a test execution automation process that can be adapted to work with the models.\n\n\\subsection{The Modeling Maturity Level}\n\nAn organization may also want to considered their current Modeling Maturity Level, especially with the aim of determining correctly the appropriate amount of investment that has to be made on personnel training. It consists on five levels:\n\nLevel 0: No Specification.\n\nLevel 1: Textual:Consists of informal, natural-language documents.\n\nLevel 2: Text with Diagrams: High level diagrams complement the text specifications.\nLevel 3:  Models with Text.\n\nLevel 4:  Precise Models: Recommended level for Model Driven Architecture approaches. The model has a precise meaning, and some code is automatically generated from the model.\n\nLevel 5:  Models Only:the model is used like a high-level programming language.\n\nSince the models created for MBT don’t have to be as detailed as the ones for MDT, a level 3 maturity should be enough for an organization to feel confident in their implementation.\n\n\\section{Limitations of Model-Based Testing}\nModel-Based testing suffers from the same limitation that all testing approaches, since it is impossible to guarantee that all the defects present in the system will be found.This would require all the possible paths, variable combinations and environments to be tested, and that would involve an almost infinite amount of tests.\n\nMBT is also mostly used for functional testing, which means that its implementation will not help with the testing of non-functional requirements. This types of testing include performance, volume, stress, security, compatibility, usability and scalability testing.\n\nSome applications are more suited for MBT than others. Applications whose execution cannot be automated, or that require human interaction are probably not going to benefit as much from an MBT implementation. In these cases the benefits may come from producing a smaller, more efficient test suite.\n\n\n\\section{Model-Based Testing in Agile Development environments}\n\\subsection{Test-Driven Development}\nIn some Agile methods it is a good practice to write the unit tests for a component before the component itself, so the development can be guided since the beginning to satisfy the tests, and so they can be run every time the code is modified or refactored. These tests can also be used to document the code, and make regression testing easier. Model-Based testing can be used to rapidly generate these tests, ensuring a better quality test suite.\n\\subsection{Generation of Acceptance Tests}\n\nIn agile methodologies such as Extreme Programming it is important for the customer on site to write first acceptance tests to drive the goals of the iteration. Model-based testing help to generate better acceptance tests faster. A model can be created from the user stories or an existing model can be evolved with the new functionalities, and the testers then generate from it an adequate acceptance test suite.\nModeling the requirements, as mentioned in previous sections, helps to quickly identify potential errors, inconsistencies or holes in them. \n\n\n\\subsection{Agile Modeling}\n\nAgile Modeling is a methodology that seeks to implement agile values, principles and practices to software modeling. Some of these principles can be applied to the development of modelling for testing. They include the encouragement of model simplicity, iterative approaches, and  a focus on the generated test scripts from the model, instead of creating models just for documentation purposes.\n\n\n\n\n\\section{Survey of tools}\n\nA short overview of 3 Model-Based tools will be provided.\n\\subsection{fMBT}\nfMBT is an open source tool that provides a model editor, test generator, generic adapters for testing through various interfaces, and tools for analyzing logs. It runs on generic Linux platforms, and supports steps written in C++, Python, JavaScript and shell script. fMBT can also be easily integrated into existing regression test suites or continuous integration systems.\n\n\n\\subsection{MaTeLo}\nMaTeLo is a Model-Based testing suite created by All4Tec. Per their website, it offers “a complete solution of Model-Based Testing composed of a powerful environment and a user-friendly interface in order to ease your validation activities.” It provides a graphical test design tool, automated test case and test script generation and requirement traceability. It also facilitates online collaborative work.\n\n\n\n\\subsection{4test}\n4test is a cloud based tool that provides support for DEVOPS. It includes tools to easily generate acceptance criteria based on specifications written in a  Gherkin-based syntax, which is easier for the customer on site and the other stakeholders to understand. It also automates the test design and script generation.\n\n\\section{Success case studies}\nIn this section two cases from successful implementation of MBT in critical systems will be presented.\n\n\\subsection {MBT application in NASA’s OSAL}\nChristoph Schulze, Dharmalingam Ganesan and Mikael Lindvall Fraunhofer of the Center for Experimental Software Engineering College Park presented a case study that evaluated model-based testing in NASA’s Operating System Abstraction Layer (OSAL with the objective of evaluating its applicability and effectiveness. The models were developed using FSMs and from there they generated tests automatically. After the test execution a few previously unknown defects were found.\n\n\\subsection {Modeling the environment of the Seismic RTES}\n\nDusica Marijan from the Simula Research Laboratory presented in the IEEE 23rd International Symposium on Software Reliability Engineering Workshops a case study of using Model-Based testing to test  the software for seismic real-time embedded system (RTES) in a simulated environment. The models were created using UML and OCL notations for the various constraints and limitations of the environment. The author acknowledges the obstacles presented in their attempt to introduce MBT to the test teams. The greatest hurdle was the training of the testing team, since they were trained in the traditional testing approaches. Limited resources and pressure for quick delivery were also obstacles that had to be dealt with for the implementation.\n\n\\section{Future Work and limitations}\nThis document, for scope restrictions, doesn't go in-depth enough in the model creation process. Only the two must popular approaches are presented, but there are many others with are better suited for more specific cases. This section could be expanded further by providing more details on how a set of informal requirements should be abstracted and transformed into a model, and how to choose the appropriate level of abstraction. More concrete examples of the usage of the most common modeling languages could also be added. OCL and B in particular are very interesting notation languages that significantly increase the modeling capabilities of UML and FSMs, and their usage and syntax could be also expanded further. \n\n\n\n\n\n\n\n\\section{Conclusions}\nModel-Based testing allows organizations to take test automation to a new level by automating the generation of the black box tests as well. This changes the paradigm of the tester’s role, increasing their involvement in the software design stages and introducing testing in a much earlier stage.There are many benefits that organizations can get from pursuing a Model-Based testing implementation, including the creation of a better, streamlined test suite that pre-satisfies the coverage criteria determined by the requirements and the automatic generation, execution and administration of test scripts and test results.\n\nHowever, before going through with such an implementation, the interested organizations must consider their own test and modeling maturity levels, to avoid introducing too much disruption to their software development process.\nModel-Based testing can be useful in agile development by providing a practical way to generate high quality acceptance tests.\n\n\n\n\\end{document}\n",
			"settings":
			{
				"buffer_size": 29917,
				"line_ending": "Unix",
				"name": "% This is samplepaper.tex, a sample chapter demons"
			}
		},
		{
			"contents": "% This is samplepaper.tex, a sample chapter demonstrating the\n% LLNCS macro package for Springer Computer Science proceedings;\n% Version 2.20 of 2017/10/04\n%\n\\documentclass[runningheads]{llncs}\n%\n\\usepackage{graphicx}\n% Used for displaying a sample figure. If possible, figure files should\n% be included in EPS format.\n%\n% If you use the hyperref package, please uncomment the following line\n% to display URLs in blue roman font according to Springer's eBook style:\n% \\renewcommand\\UrlFont{\\color{blue}\\rmfamily}\n\n\\begin{document}\n%\n\\title{An Overview of Model-Based Testing}\n%\n%\\titlerunning{Abbreviated paper title}\n% If the paper title is too long for the running head, you can set\n% an abbreviated paper title here\n%\n\\author{Jorge Villalobos Rebollosa\\orcidID{1622886}}\n%\n\n\n% First names are abbreviated in the running head.\n% If there are more than two authors, 'et al.' is used.\n%\n\\institute{Chair of Software Engineering, University of Mannheim, Mannheim, Germany }\n%\n\\maketitle              % typeset the header of the contribution\n%\n\\begin{abstract}\nThe purpose of this document is to provide an overview of Model-Based testing, an approach that seeks to automate the generation of black box tests. The traditional testing approaches will be explained and contrasted with model-based testing. The model-based testing process will be explained in four  phases: Model creation, abstract case generation, concretization and execution and evaluation. \n\nThe organizational requirements for a model-based testing implementation will also be explored. There will be an analysis on the ways in which model-based testing can support agile development methodologies. And finally, a survey of popular model-based testing tools will be provided, and case studies of successful model-based testing implementations will be discussed.\n\n\\keywords{Testing  \\and Modeling \\and Model-Based Testing \\and Agile Development}\n\\end{abstract}\n%\n%\n%\n\\section{Introduction}\nWith the increasing adoption of Agile development methodologies and the shortening of the development cycles it has become imperative for software testers to find  better and more efficient ways to evaluate the quality of software products~\\cite{c1}. The automation of test execution has reduced significantly the time required for testing while also increasing the re-usability of the generated test suite, but still it depends on the tester to generate the right test cases that satisfy the quality requirements for the system or module. This has led to the development of approaches that seek to automate the test generation process, the most common of which is known as Model-based testing.\n\\section{Model-Based Testing}\n\n\\subsection{The testing process}\n\nThe most basic task of the software tester is to generate a set of test cases (known also as Test Suite) and then run those tests in the System Under Test (SUT) in order to evaluate the quality of the system against the informal requirements~\\cite{c2}. For the tester to be able to evaluate whether a test passed successfully or not, a Test Oracle is required, which is a list, method or system that provides the correct results or behavior of a system in response to a specific set of inputs~\\cite{c3}. \n\nIdeally the tester will create an exhaustive set of tests that go through every aspect of the program and verify that every possible input produces the required state. In reality though, this is impossible, since the amount of tests could be infinite. Moreover, systems are developed under strict  time and budget deadlines, so only a fractional amount of the possible test cases can be executed before release. It is therefore the job of the tester to determine which set of test cases should be executed, and to provide information on which parts of the overall system have been evaluated, and which parts haven’t. ~\\cite{c4}\n\nThe execution of the previously generated tests is  a boring and repetitive task, and can also be very expensive since it requires a lot of human effort. It is also prone to human errors. To tackle this several test automation tools exist in the market.~\\cite{c5} The goal of test automation is for the tester to use a programming language to define the test once, and then execute these test automatically whenever necessary. This allows for concepts such as Continuous Integration to be possible, since developers can now automatically trigger the regression test suite every time they push a new version of the module they are working on.\n\nHowever, the test automation of the test execution still depends on the tester to design the test suite, which means there is still the need for a test oracle, and there is still the problem of generating the right tests to ensure the desired coverage. Automated test suites also have the disadvantage of not being very flexible, since small changes on the way the system works (A change in the system’s API for example) renders a lot of tests useless.\n\n\\subsection{Model-Based Testing}\n\nNew testing approaches allow us to  take automation one step further. Instead of having the tester generate the test suite by himself, the role of the tester is now to determine an abstract version of the system based on the requirements, and then allowing tools to automatically generate a set of tests from that version. This allows the tester to automatically generate a set of tests that satisfy the desired coverage. This new role also allows the tester to perform an evaluation of the informal requirements, so ambiguous or incomplete requirements can also be quickly found. As a result, the testing process can now begin before a single line of code is written by the developers, and finding errors in the requirement gathering/design phase of the software development life cycle ends up saving a lot of time and money. This is where the Model-based testing comes into play.\n\nModel-Based Testing can therefore be defined as the “automation of the design of black box tests”.~\\cite{c6} It is an application of concepts derived from the more general Model Driven Development (MDD), and can take advantage of most of the same (or similar) modeling languages. \n\n\\subsection{ Modeling for testing}\nEvery system developed has as a starting point a set of informal requirements, provided by the client or one of the shareholders. The client usually has no knowledge of how a system is built, and as such the requirements are usually expressed in a non technical language, in terms of desired results or functionality. Therefore, before a system can be built it is necessary to transform this requirements into something the developers can actually use to begin building the system. Models provide a solution for this process. Software models are abstractions of the required functionality of a system, taking advantage of visual techniques or an abstract language to represent the intended design.\n\nThere are several object modeling languages and techniques that can be used to create these models, one of the most common being the Universal Modeling Language (UML).~\\cite{c7} \nAn important objective of modeling is to identify ambiguities in the requirements, as well as defining the scope of the implementation.\n\n\\subsection{ Differences between Modeling for Development and Modeling for testing}\nIt might be tempting to reuse the same models created by the development team for testing purposes. However this is not considered a good practice. Models created for developing tend to have way too much detail, especially regarding low-level implementation issues, which may not be relevant for testing. These models also tend to to not be very specific regarding data processing and manipulation, and focus instead on the structure of the SUT.~\\cite{c8}\n\nThere are therefore two characteristics that a good model should satisfy for test generation: They must be small in comparison to the scope of the whole system, which reduces their cost and complexity, but should be detailed enough that they include all the characteristics relevant to the desired test. It is the job of a good test modeler to find this balance, and it is not very common to find models created for development that satisfy these conditions.\nFinally, using the same models as the development team would mean that any error injected in the design phase or in the requirements will persist, since the tests are going to be applied to the same incorrect model.\n\n\\section{The Model-Based testing process}\nThe process of creating the appropriate tests from a set of requirements consists on the following steps:\n\n\\subsection{Model Definition}\nPrevious to this step a list of formal requirements should be provided. The requirements encompass all the desired functionality the shareholders want from the system. These requirements are sometimes ambiguous, incomplete or contradictory. It is therefore important for the tester to be included in the requirement gathering phase, so these errors can be identified quicker. \n\nNext the level of abstraction should be decided. This is done with the objective of reducing the detail of the model as much as possible while still covering the test objectives. Next the data, operations, outputs and the interactions of the module with other systems or subsystems have to be considered. Unless different input parameters change the behavior of a particular operation, or produce a change in state, it is recommended to leave them out of the model for simplicity. Input parameters are better dealt with once the abstract tests have been generated and being translated into executable tests.~\\cite{c9}\nOutputs of operations should only be included if their value can be used for the test oracle. \n\nThe behavior of the model can be different than the one of the SUT. Complex operations can be split into smaller steps for a more in depth test of the process. Sequence of operations in the SUT can also be aggregated into a single operation in the model if the result is the only aspect being tested.~\\cite{c10}\n\nNext it is necessary to define the notation language that is going to be used to express the model. These can be classified into two major paradigms: Transition-based models and State-based (also known as pre/post) notations. A brief overview of each is given below.\n\n\\subsubsection{Transition based models}\n With this type of notation the SUT is represented as a series of states and the transitions between them. If determined correctly, each state in the model should result in a change in behavior of the system.Usually these models are expressed using nodes to represent the states and arrows to represent the different transitions. The most simple example of transition based models are finite state machines (FSMs), although there are other languages such as UML State Machines that allow to expand the models with variables, hierarchies or parallelism between machines. Transition based models are recommended for control-oriented systems.\n\n\\subsubsection{Pre/Post Models}\nThis type of notation models the SUT using variables to represent the internal state of the system and the operations that modify them. These operations are defined by preconditions and post-conditions. Some of the most used examples of this paradigm are the B notation and UML Object Constraint Language (OCL)~\\cite{c11}.Pre/Post models are recommended for data oriented systems.~\\cite{c12}\n\n\nFinally the model should be verified and validated. Verification concerns making sure the model has been created correctly, while validation checks that the model does represent the aspects of the SUT that need to be tested.~\\cite{c13}\n\n\n\\subsection{Generation of abstract test cases}\n\nIt is in this stage that the tester should determine the appropriate selection criteria to generate the necessary tests from the model. \n\n\\subsubsection{Coverage Criteria}\n\nA simple benchmark for test generation can be, for example, to generate tests such as all the requirements have been covered (Requirements-based criteria). Selection criteria can also be based on the coverage criteria usually used in white box testing, but applied in a prescriptive way. The three most relevant types of coverage ate Statement Coverage (SC), Decision Coverage (DC) and Path Coverage (PC).To achieve statement coverage, every single state within the system should be reached at one point by the tests. Decision Coverage, as it name suggests, is concerned with every decision, and to achieve it the tests must follow every option on every decision at least once. Satisfying DC also implies satisfying SC.Finally, the strongest criteria, Path Coverage, requires every single satisfiable path to be traversed by the tests.~\\cite{c13}\n These can be subdivided into structural model criteria, which seek to achieve coverage of the control-flow, and data coverage criteria, which aim to cover the input data space of an operation or a transition.\n\n\\subsubsection{Domain Specific and Statistical Criteria}\nThe tester may also want to apply his previous experiences and his knowledge of the domain space to explicitly specify the set of tests that should be generated, or to focus the tests on a specific kind of fault. Finally, random generation can also be used as a criteria, especially when there is a limited amount of time and a general overview of the behaviors of the system is desired.\n\n\\subsubsection{Selecting the Right Criteria}\nThe amount of tests generated, and the time it will take the tool to do so are influenced on the selected criteria.It is therefore recommended to select different selection criteria depending on the part of the SUT that is being tested. The most commonly used tools for test generation are flexible enough to allow this.  Some examples are LEIRIOS Test Generator ~\\cite{c14}(Which generates tests from models that use the B notation), and Qtronic.~\\cite{c15}\nStandards and regulations regarding the minimum of coverage required for some specific domain spaces should also be considered.~\\cite{c16}\n\n\n\n\\subsection{Concretization}\nAt this stage it is assumed that a working version of the part of the SUT that needs to be tested is already created and initialized.\n\nThe output of the test generation stage is a suite of abstract test cases, meaning that they cannot be directly executed in the SUT. This increases the reusability and flexibility of the tests, but creates the necessity of bridging the gap between the abstract model and the SUT. \n\nIn this section, the further work that needs to be done in order to transform an abstract test case into an executable one will be reviewed. This process is known as concretization. \nThere are two main approaches for concretization of a test suite: the adaptation approach and the transformation approach. \n\n\\subsubsection{The Adaptation Approach}\n\nThe adaptation approach involves creating an adapter code that interprets the operations of the model and executes them in the SUT. First, model-level operations and abstract input values are converted into concrete SUT operations. These are then executed in the SUT and the outputs (or any other relevant results) are captured. These are then converted back again into abstract values, which are then fed back into the model. The model’s rules then are used to determine whether the test passed or not. These transformations are done in real time, as the abstract test suite is interpreted. The adaptation approach is recommended when dealing with online testing, where the adapter’s API can provided a tightly integrated two-way connection between the SUT interface and the model.~\\cite{c17}\n\n\\subsection{The Transformation Approach}\n\nThe transformation approach involves creating executable test scripts out of the abstract tests. These can be created in standard programming or test scripting languages. These tests can then be executed and evaluated by the testing team. The transformation approach can be very useful for organizations that already have a mature testing process and therefore have already defined an automated test execution framework.~\\cite{c18} The right tool can then be used to adapt the tests created by the model directly into the existent framework. These generated tests can follow the same organization and version management standards currently defined. In this case the adoption of model-based testing would be less disruptive, since the current test management tools, repositories, test execution platforms, and test reporting processes don’t have to be changed.\nHowever, if the organization’s processes are not mature enough then it is very important to first define an appropriate structure to the test suite, so the hundreds (or even thousands) of generated tests can be tracked. Naming conventions, folders and hierarchies should be applied. Treaceability from the test scripts, the abstract tests and the original requirements must also be maintained.\n\nA third option is to apply a mix of the two approaches. An adapter code can be added to the SUT with the purpose of raising the abstraction level, bringing it closer to the abstraction of the model and making the transformation easier.\n\n\\subsection{Execution and Analysis}\nFinally the tests can be executed and the results can be analyzed. Some model-based testing tools can also handle this phase ~\\cite{c19}, or organizations can use the tools they already have for managing the execution and documentation of test scripts.  It is important to note that a failed test can also be caused due to an error in the adapter or in the model itself. During the initial runs of the test suite a lot of feedback about the correctness of the model is obtained, and a higher amount of errors are expected. The testers should then use this feedback to improve the quality of the model, correct adapter errors or even identify flaws in the requirements.\n\n\\section{Advantages of model based testing}\nIn this section the advantages that adopting model-based testing can bring to organizations will be discussed.\n\nReduced cost and time\nModel-based testing can have a significant impact in the testing time if the effort to create and maintain the model is lower than the effort directed to manually create the test suite. There are also several time saving advantages after the tests have been executed, during the analysis and debugging phase. Generated tests report failures in a more consistent way and some tools provide the capability of finding the shortest test sequence that produces a failure, which makes debugging easier. A successful model-based testing implementation will result in the automation of the majority of the testing process, and subsequently results in a decrease in development costs.\n\n\n\n\\subsection{Improved test quality}\n\nManually deriving test cases from requirements is a laborious and error-prone process whose success depends on the ingenuity of the testers. Manually scripted tests are also software programs and are therefore also vulnerable to error injection, meaning that the test scripts themselves have to be tested as well. Model-based testing approaches therefore produce more consistent and reliable tests.\nManually creating a test suite that satisfies the desired coverage can be almost impossible, since even simple programs can involve a very large amount of paths. An industry survey by CA technologies in 2015 found that over 42 percent of the participants “cited a lack of test coverage creating defects and rework as a main software challenge.”~\\cite{c20}\nModel-based testing also produces more optimized tests, and the resulting test suite usually contains the smallest amount of paths needed for maximum coverage. \n\n\n\\subsection{Requirements defect detection}\n\nWriting a model can also expose issues with the requirements. Incomplete, ambiguous or contradictory requirements become more apparent when translated to the precise semantics required by the model. Errors found in the requirements will not be carried over to the design and implementation phases. This is a major benefit since the cost of fixing errors in software products increases exponentially by every phase of the development process.~\\cite{c21}\n\n\\subsection{Traceability}\n\nModel-based testing also provides the ability to quickly relate every test to the model and to the requirements. This is called a traceability matrix. If requirements change and the model evolves, traceability becomes very useful for optimization since it allows the tester to focus on the execution of the tests related to the changes.~\\cite{c22}\n\n\n\n\\section{Organizational requirements for Successful Model-Based Testing implementations}\nThe implementation of Model-Based testing practices in an organization will imply a significant change in paradigm in the way the organization approaches Software quality, and will most likely involve additional costs in personnel training, tool acquisitions and process re-engineering. Therefore, in order to increase the probability of a successful MBT implementation some factors should be considered.\n\nThe first pre-requisite is for the organization to have in place a mature and well defined software development process, with well established channels of communication between the testing team and the rest of the project’s shareholders. ~\\cite{c23}\n\n\\subsection{The Test Maturity Model}\n\nIn order to assess the testing maturity level of organizations, the Illinois Institute of Technology developed the Test Maturity Model~\\cite{c24}, which is based on the Capability Maturity Model but focuses on the testing process and the monitoring of quality. It is also divided into five levels of maturity. They are the following:\n\nLevel 1: Initial The organization lacks resources, tools, and trained staff.\n\nLevel 2: Phase Definition: Basic testing methods and techniques are in place.\n\nLevel 3: Integration: Testing is integrated into an entire life cycle, and the roles of tester and developer are separated.\n\nLevel 4: Management and Measurement Testing is a measured and quantified process, defects are logged and given severity levels.\n\nLevel 5: Optimized Testing is continuously improved, the organization invests in tools to support the design of test cases and the administration of defects.\n\nA level 5 Test maturity level is required for a Model-Based testing implementation, since it involves the adoption of new tools and the improvement of the testing process. It is also recommended that the organization has already in place a test execution automation process that can be adapted to work with the models.\n\n\\subsection{The Modeling Maturity Level}\n\nAn organization may also want to considered their current Modeling Maturity Level ~\\cite{c25}, especially with the aim of determining correctly the appropriate amount of investment that has to be made on personnel training. It consists on five levels:\n\nLevel 0: No Specification.\n\nLevel 1: Textual:Consists of informal, natural-language documents.\n\nLevel 2: Text with Diagrams: High level diagrams complement the text specifications.\nLevel 3:  Models with Text.\n\nLevel 4:  Precise Models: Recommended level for Model Driven Architecture approaches. The model has a precise meaning, and some code is automatically generated from the model.\n\nLevel 5:  Models Only:the model is used like a high-level programming language.\n\nSince the models created for MBT don’t have to be as detailed as the ones for MDT, a level 3 maturity should be enough for an organization to feel confident in their implementation.\n\n\\section{Limitations of Model-Based Testing}\nModel-Based testing suffers from the same limitation that all testing approaches, since it is impossible to guarantee that all the defects present in the system will be found.This would require all the possible paths, variable combinations and environments to be tested, and that would involve an almost infinite amount of tests.\n\nMBT is also mostly used for functional testing, which means that its implementation will not help with the testing of non-functional requirements. This types of testing include performance, volume, stress, security, compatibility, usability and scalability testing.\n\nSome applications are more suited for MBT than others. Applications whose execution cannot be automated, or that require human interaction are probably not going to benefit as much from an MBT implementation. In these cases the benefits may come from producing a smaller, more efficient test suite.\n\n\n\n\n\\section{Model-Based Testing in Agile Development environments}\nThere are several ways in which Model-Based testing can be implemented by organizations that use agile based methodologies to improve their processes.There is also a developing trend called agile modeling, which intends to adapt agile principles to the modeling process.\n\n\\subsection{Test-Driven Development}\nIn some agile methods it is a good practice to write the unit tests for a component before the component itself, so the development can be guided since the beginning to satisfy the tests, and so they can be run every time the code is modified or refactored. These tests can also be used to document the code, and make regression testing easier. Model-Based testing can be used to rapidly generate these tests, ensuring a better quality test suite.~\\cite{c25}\n\\subsection{Generation of Acceptance Tests}\n\nIn agile methodologies such as Extreme Programming it is important for the customer on site to write first acceptance tests to drive the goals of the iteration. ~\\cite{c26} Model-based testing help to generate better acceptance tests faster. A model can be created from the user stories or an existing model can be evolved with the new functionality, and the testers then generate from it an adequate acceptance test suite.\nModeling the requirements, as mentioned in previous sections, helps to quickly identify potential errors, inconsistencies or holes in them.\n\n\n\\subsection{Agile Modeling}\n\nAgile Modeling ~\\cite{c27}is a methodology that seeks to implement agile values, principles and practices to software modeling. Some of these principles can be applied to the development of modelling for testing. They include the encouragement of model simplicity, iterative approaches, and  a focus on the generated test scripts from the model, instead of creating models just for documentation purposes.\n\n\n\n\n\\section{Survey of tools}\n\nA short overview of 3 Model-Based tools will be provided.\n\\subsection{fMBT}~\\cite{c28}\nfMBT is an open source tool that provides a model editor, test generator, generic adapters for testing through various interfaces, and tools for analyzing logs. It runs on generic Linux platforms, and supports steps written in C++, Python, JavaScript and shell script. fMBT can also be easily integrated into existing regression test suites or continuous integration systems.\n\n\n\\subsection{MaTeLo}~\\cite{c29}\nMaTeLo is a Model-Based testing suite created by All4Tec. Per their website, it offers “a complete solution of Model-Based Testing composed of a powerful environment and a user-friendly interface in order to ease your validation activities.” It provides a graphical test design tool, automated test case and test script generation and requirement traceability. It also facilitates online collaborative work.\n\n\n\n\\subsection{4test}~\\cite{c30}\n4test is a cloud based tool that provides support for DEVOPS. It includes tools to easily generate acceptance criteria based on specifications written in a  Gherkin-based syntax, which is easier for the customer on site and the other stakeholders to understand. It also automates the test design and script generation.\n\n\\section{Success case studies}\nIn this section two cases from successful implementation of MBT in critical systems will be presented.\n\n\\subsection {MBT application in NASA’s OSAL}\nChristoph Schulze, Dharmalingam Ganesan and Mikael Lindvall Fraunhofer of the Center for Experimental Software Engineering College Park presented a case study that evaluated model-based testing in NASA’s Operating System Abstraction Layer (OSAL with the objective of evaluating its applicability and effectiveness.~\\cite{c31} The models were developed using FSMs and from there they generated tests automatically. After the test execution a few previously unknown defects were found.\n\n\\subsection {Modeling the environment of the Seismic RTES}\n\nDusica Marijan from the Simula Research Laboratory presented in the IEEE 23rd International Symposium on Software Reliability Engineering Workshops a case study of using Model-Based testing to test  the software for seismic real-time embedded system (RTES) in a simulated environment.~\\cite{c32} The models were created using UML and OCL notations for the various constraints and limitations of the environment. The author acknowledges the obstacles presented in their attempt to introduce MBT to the test teams. The greatest hurdle was the training of the testing team, since they were trained in the traditional testing approaches. Limited resources and pressure for quick delivery were also obstacles that had to be dealt with for the implementation.\n\n\\section{Future Work and limitations}\nThis document, for scope restrictions, doesn't go in-depth enough in the model creation process. Only the two must popular approaches are presented, but there are many others with are better suited for more specific cases. This section could be expanded further by providing more details on how a set of informal requirements should be abstracted and transformed into a model, and how to choose the appropriate level of abstraction. More concrete examples of the usage of the most common modeling languages could also be added. OCL and B in particular are very interesting notation languages that significantly increase the modeling capabilities of UML and FSMs, and their usage and syntax could be also expanded further. \n\n\n\n\n\n\n\n\\section{Conclusions}\nModel-Based testing allows organizations to take test automation to a new level by automating the generation of the black box tests as well. This changes the paradigm of the tester’s role, increasing their involvement in the software design stages and introducing testing in a much earlier stage.There are many benefits that organizations can get from pursuing a Model-Based testing implementation, including the creation of a better, streamlined test suite that pre-satisfies the coverage criteria determined by the requirements and the automatic generation, execution and administration of test scripts and test results.\n\nHowever, before going through with such an implementation, the interested organizations must consider their own test and modeling maturity levels, to avoid introducing too much disruption to their software development process.\nModel-Based testing can be useful in agile development by providing a practical way to generate high quality acceptance tests.\n\n\nFor citations of references, we prefer the use of square brackets\nand consecutive numbers. Citations using labels or the author/year\nconvention are also acceptable. The following bibliography provides\na sample reference list with entries for journal\narticles~\\cite{art1}, an LNCS chapter~\\cite{ref_lncs1}, a\nbook~\\cite{ref_book1}, proceedings without editors~\\cite{ref_proc1},\nand a homepage~\\cite{ref_url1}. Multiple citations are grouped\n\\cite{ref_article1,ref_lncs1,ref_book1},\n\\cite{ref_article1,ref_book1,ref_proc1,ref_url1}.\n%\n% ---- Bibliography ----\n%\n% BibTeX users should specify bibliography style 'splncs04'.\n% References will then be sorted and formatted in the correct style.\n%\n% \\bibliographystyle{splncs04}\n% \\bibliography{mybibliography}\n%\n\\begin{thebibliography}{8}\n\n\\bibitem{c1}\n\\bibitem{c2}\n\\bibitem{c3}\n\\bibitem{c4}\n\\bibitem{c5}\n\\bibitem{c6}\n\\bibitem{c7}\n\\bibitem{c8}\n\\bibitem{c9}\n\\bibitem{c10}\n\\bibitem{c11}\n\\bibitem{c12}\n\\bibitem{c13}\n\\bibitem{c14}\n\\bibitem{c15}\n\\bibitem{c16}\n\\bibitem{c17}\n\\bibitem{c18}\n\\bibitem{c19}\n\\bibitem{c20}\n\\bibitem{c21}\n\\bibitem{c22}\n\\bibitem{c23}\n\\bibitem{c24}\n\n\\bibitem{c25}\nKleppe, A., Warmer, J., Bastj, W.:MDA Explained: The Model Driven Architecture : Practice and Promise Addison-Wesley Professional, USA (2003)\n\\bibitem{c26}\nExtreme programming homepage.\\url{http://www.extremeprogramming.org/rules/functionaltests.html}. Last accessed 12 May 2019\n\\bibitem{c27}\nAgile Modeling Homepage. \\url{http://agilemodeling.com/essays/introductionToAM.htm}. Last accessed 12 May 2019\n\\bibitem{c28}\nFMBT homepage.  \\url{https://01.org/fmbt/overview}. Last accessed 12\nMay 2019\n\\bibitem{c29}\nALL4TEC homepage, \\url{https://www.all4tec.com/}. Last accessed 12\nMay 2019\n\\bibitem{c30}\n4Test homepage, \\url{hhttps://4test.io/}. Last accessed 12\nMay 2019\n\\bibitem{c31}\nSchulze, C, Dharmalingam, G, Lindvall, M: Model-based Testing of NASA’s OSAL API. 2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE), pp. 300-309 (2013)\n\n\\bibitem{c32}\nMarijan, Dusica: A Review of Two Experiences from Applying Model Based Testing in Practice. 2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops(2012)\n\n\n\n\n\n\\bibitem{art1}\nAuthor, F.: Article title. Journal \\textbf{2}(5), 99--110 (2016)\n\\bibitem{ref_lncs1}\nAuthor, F., Author, S.: Title of a proceedings paper. In: Editor,\nF., Editor, S. (eds.) CONFERENCE 2016, LNCS, vol. 9999, pp. 1--13.\nSpringer, Heidelberg (2016). \\doi{10.10007/1234567890}\n\n\\bibitem{ref_book1}\nAuthor, F., Author, S., Author, T.: Book title. 2nd edn. Publisher,\nLocation (1999)\n\nKleppe, A., Warmer, J., Bastj, W.:MDA Explained: The Model Driven Architecture : Practice and Promise Addison-Wesley Professional, USA (2003\n\n\\bibitem{ref_proc1}\nAuthor, A.-B.: Contribution title. In: 9th International Proceedings\non Proceedings, pp. 1--2. Publisher, Location (2010)\n\n\\bibitem{ref_url1}\n4Test homepage, \\url{hhttps://4test.io/}. Last accessed 12\nMay 2019\n\\end{thebibliography}\n\\end{document}\n",
			"settings":
			{
				"buffer_size": 33815,
				"line_ending": "Unix",
				"name": "% This is samplepaper.tex, a sample chapter demons"
			}
		},
		{
			"file": "/Users/Wolvesville/Downloads/melanee_web_project--master/src/main/webapp/WEB-INF/views/tool.jsp",
			"settings":
			{
				"buffer_size": 90610,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "import React, { Component } from \"react\";\nimport {\n  mxGraph,\n  mxParallelEdgeLayout,\n  mxConstants,\n  mxEdgeStyle,\n  mxLayoutManager,\n  mxGraphHandler,\n  mxGuide,\n  mxEdgeHandler,\n  mxCell,\n  mxGeometry,\n  mxRubberband,\n  mxDragSource,\n  mxKeyHandler,\n  mxCodec,\n  mxClient,\n  mxConnectionHandler,\n  mxUtils,\n  mxToolbar,\n  mxEvent,\n  mxImage,\n  mxConstraintHandler,\n  mxFastOrganicLayout,\n  mxUndoManager,\n  mxObjectCodec,\n  mxHierarchicalLayout,\n  mxConnectionConstraint,\n  mxCellState,\n  mxPoint,\n  mxGraphModel,\n  mxPerimeter,\n  mxCompactTreeLayout,\n  mxCellOverlay\n} from \"mxgraph-js\";\n\nclass Toolbar extends Component {\n  state = {};\n  render() {\n    mxConnectionHandler.prototype.connectImage = new mxImage(\n      \"images/connector.gif\",\n      16,\n      16\n    );\n\n    // Checks if browser is supported\n    if (!mxClient.isBrowserSupported()) {\n      // Displays an error message if the browser is\n      // not supported.\n      mxUtils.error(\"Browser is not supported!\", 200, false);\n    } else {\n      // Creates the div for the toolbar\n      var tbContainer = document.createElement(\"div\");\n      tbContainer.style.position = \"absolute\";\n      tbContainer.style.overflow = \"hidden\";\n      tbContainer.style.padding = \"2px\";\n      tbContainer.style.left = \"0px\";\n      tbContainer.style.top = \"50px\";\n      tbContainer.style.width = \"24px\";\n      tbContainer.style.bottom = \"0px\";\n\n      document.body.appendChild(tbContainer);\n\n      // Creates new toolbar without event processing\n      var toolbar = new mxToolbar(tbContainer);\n      toolbar.enabled = false;\n\n      // Creates the div for the graph\n      var container = document.createElement(\"div\");\n      container.style.position = \"absolute\";\n      container.style.overflow = \"hidden\";\n      container.style.left = \"24px\";\n      container.style.top = \"50px\";\n      container.style.right = \"0px\";\n      container.style.bottom = \"0px\";\n      container.style.background = 'url(\"editors/images/grid.gif\")';\n\n      document.body.appendChild(container);\n\n      // Workaround for Internet Explorer ignoring certain styles\n      if (mxClient.IS_QUIRKS) {\n        document.body.style.overflow = \"hidden\";\n        new mxDivResizer(tbContainer);\n        new mxDivResizer(container);\n      }\n\n      // Creates the model and the graph inside the container\n      // using the fastest rendering available on the browser\n      var model = new mxGraphModel();\n      var graph = new mxGraph(container, model);\n      graph.dropEnabled = true;\n\n      // Matches DnD inside the graph\n      mxDragSource.prototype.getDropTarget = function(graph, x, y) {\n        var cell = graph.getCellAt(x, y);\n\n        if (!graph.isValidDropTarget(cell)) {\n          cell = null;\n        }\n\n        return cell;\n      };\n\n      // Enables new connections in the graph\n      graph.setConnectable(true);\n      graph.setMultigraph(false);\n\n      // Stops editing on enter or escape keypress\n      var keyHandler = new mxKeyHandler(graph);\n      var rubberband = new mxRubberband(graph);\n\n      var addVertex = function(icon, w, h, style) {\n        var vertex = new mxCell(null, new mxGeometry(0, 0, w, h), style);\n        vertex.setVertex(true);\n\n        addToolbarItem(graph, toolbar, vertex, icon);\n      };\n\n      addVertex(\n        \"editors/images/swimlane.gif\",\n        120,\n        160,\n        \"shape=swimlane;startSize=20;\"\n      );\n      addVertex(\"editors/images/rectangle.gif\", 100, 40, \"\");\n      addVertex(\"editors/images/rounded.gif\", 100, 40, \"shape=rounded\");\n      addVertex(\"editors/images/ellipse.gif\", 40, 40, \"shape=ellipse\");\n      addVertex(\"editors/images/rhombus.gif\", 40, 40, \"shape=rhombus\");\n      addVertex(\"editors/images/triangle.gif\", 40, 40, \"shape=triangle\");\n      addVertex(\"editors/images/cylinder.gif\", 40, 40, \"shape=cylinder\");\n      addVertex(\"editors/images/actor.gif\", 30, 40, \"shape=actor\");\n      toolbar.addLine();\n\n      var button = mxUtils.button(\n        \"Create toolbar entry from selection\",\n        function(evt) {\n          if (!graph.isSelectionEmpty()) {\n            // Creates a copy of the selection array to preserve its state\n            var cells = graph.getSelectionCells();\n            var bounds = graph.getView().getBounds(cells);\n\n            // Function that is executed when the image is dropped on\n            // the graph. The cell argument points to the cell under\n            // the mousepointer if there is one.\n            var funct = function(graph, evt, cell) {\n              graph.stopEditing(false);\n\n              var pt = graph.getPointForEvent(evt);\n              var dx = pt.x - bounds.x;\n              var dy = pt.y - bounds.y;\n\n              graph.setSelectionCells(graph.importCells(cells, dx, dy, cell));\n            };\n\n            // Creates the image which is used as the drag icon (preview)\n            var img = toolbar.addMode(\n              null,\n              \"editors/images/outline.gif\",\n              funct\n            );\n            mxUtils.makeDraggable(img, graph, funct);\n          }\n        }\n      );\n\n      button.style.position = \"absolute\";\n      button.style.left = \"2px\";\n      button.style.top = \"2px\";\n\n      document.body.appendChild(button);\n    }\n  }\n\n  addToolbarItem(graph, toolbar, prototype, image) {\n    // Function that is executed when the image is dropped on\n    // the graph. The cell argument points to the cell under\n    // the mousepointer if there is one.\n    var funct = function(graph, evt, cell) {\n      graph.stopEditing(false);\n\n      var pt = graph.getPointForEvent(evt);\n      var vertex = graph.getModel().cloneCell(prototype);\n      vertex.geometry.x = pt.x;\n      vertex.geometry.y = pt.y;\n\n      graph.setSelectionCells(graph.importCells([vertex], 0, 0, cell));\n    };\n\n    // Creates the image which is used as the drag icon (preview)\n    var img = toolbar.addMode(null, image, funct);\n    mxUtils.makeDraggable(img, graph, funct);\n  }\n}\n\nexport default Toolbar;\n",
			"file": "toolbar.jsx",
			"file_size": 5928,
			"file_write_time": 132182667367612591,
			"settings":
			{
				"buffer_size": 5928,
				"line_ending": "Unix"
			}
		},
		{
			"contents": "\n<!--\n  Copyright (c) 2006-2013, JGraph Ltd\n  \n  Toolbar example for mxGraph. This example demonstrates using\n  existing cells as templates for creating new cells.\n-->\n<html>\n<head>\n\t<title>Toolbar example for mxGraph</title>\n\n\t<!-- Sets the basepath for the library if not in same directory -->\n\t<script type=\"text/javascript\">\n\t\tmxBasePath = '../src';\n\t</script>\n\n\t<!-- Loads and initializes the library -->\n\t<script type=\"text/javascript\" src=\"../src/js/mxClient.js\"></script>\n\n\t<!-- Example code -->\n\t<script type=\"text/javascript\">\n\t\t// Program starts here. Creates a sample graph in the\n\t\t// DOM node with the specified ID. This function is invoked\n\t\t// from the onLoad event handler of the document (see below).\n\t\tfunction main()\n\t\t{\n\t\t\t// Defines an icon for creating new connections in the connection handler.\n\t\t\t// This will automatically disable the highlighting of the source vertex.\n\t\t\tmxConnectionHandler.prototype.connectImage = new mxImage('images/connector.gif', 16, 16);\n\t\t\n\t\t\t// Checks if browser is supported\n\t\t\tif (!mxClient.isBrowserSupported())\n\t\t\t{\n\t\t\t\t// Displays an error message if the browser is\n\t\t\t\t// not supported.\n\t\t\t\tmxUtils.error('Browser is not supported!', 200, false);\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\t// Creates the div for the toolbar\n\t\t\t\tvar tbContainer = document.createElement('div');\n\t\t\t\ttbContainer.style.position = 'absolute';\n\t\t\t\ttbContainer.style.overflow = 'hidden';\n\t\t\t\ttbContainer.style.padding = '2px';\n\t\t\t\ttbContainer.style.left = '0px';\n\t\t\t\ttbContainer.style.top = '50px';\n\t\t\t\ttbContainer.style.width = '24px';\n\t\t\t\ttbContainer.style.bottom = '0px';\n\t\t\t\t\n\t\t\t\tdocument.body.appendChild(tbContainer);\n\t\t\t\n\t\t\t\t// Creates new toolbar without event processing\n\t\t\t\tvar toolbar = new mxToolbar(tbContainer);\n\t\t\t\ttoolbar.enabled = false\n\t\t\t\t\n\t\t\t\t// Creates the div for the graph\n\t\t\t\tcontainer = document.createElement('div');\n\t\t\t\tcontainer.style.position = 'absolute';\n\t\t\t\tcontainer.style.overflow = 'hidden';\n\t\t\t\tcontainer.style.left = '24px';\n\t\t\t\tcontainer.style.top = '50px';\n\t\t\t\tcontainer.style.right = '0px';\n\t\t\t\tcontainer.style.bottom = '0px';\n\t\t\t\tcontainer.style.background = 'url(\"editors/images/grid.gif\")';\n\n\t\t\t\tdocument.body.appendChild(container);\n\t\t\t\t\n\t\t\t\t// Workaround for Internet Explorer ignoring certain styles\n\t\t\t\tif (mxClient.IS_QUIRKS)\n\t\t\t\t{\n\t\t\t\t\tdocument.body.style.overflow = 'hidden';\n\t\t\t\t\tnew mxDivResizer(tbContainer);\n\t\t\t\t\tnew mxDivResizer(container);\n\t\t\t\t}\n\t\n\t\t\t\t// Creates the model and the graph inside the container\n\t\t\t\t// using the fastest rendering available on the browser\n\t\t\t\tvar model = new mxGraphModel();\n\t\t\t\tvar graph = new mxGraph(container, model);\n\t\t\t\tgraph.dropEnabled = true;\n\t\t\t\t\n\t\t\t\t// Matches DnD inside the graph\n\t\t\t\tmxDragSource.prototype.getDropTarget = function(graph, x, y)\n\t\t\t\t{\n\t\t\t\t\tvar cell = graph.getCellAt(x, y);\n\t\t\t\t\t\n\t\t\t\t\tif (!graph.isValidDropTarget(cell))\n\t\t\t\t\t{\n\t\t\t\t\t\tcell = null;\n\t\t\t\t\t}\n\t\t\t\t\t\n\t\t\t\t\treturn cell;\n\t\t\t\t};\n\n\t\t\t\t// Enables new connections in the graph\n\t\t\t\tgraph.setConnectable(true);\n\t\t\t\tgraph.setMultigraph(false);\n\n\t\t\t\t// Stops editing on enter or escape keypress\n\t\t\t\tvar keyHandler = new mxKeyHandler(graph);\n\t\t\t\tvar rubberband = new mxRubberband(graph);\n\t\t\t\t\n\t\t\t\tvar addVertex = function(icon, w, h, style)\n\t\t\t\t{\n\t\t\t\t\tvar vertex = new mxCell(null, new mxGeometry(0, 0, w, h), style);\n\t\t\t\t\tvertex.setVertex(true);\n\t\t\t\t\n\t\t\t\t\taddToolbarItem(graph, toolbar, vertex, icon);\n\t\t\t\t};\n\t\t\t\t\n\t\t\t\taddVertex('editors/images/swimlane.gif', 120, 160, 'shape=swimlane;startSize=20;');\n\t\t\t\taddVertex('editors/images/rectangle.gif', 100, 40, '');\n\t\t\t\taddVertex('editors/images/rounded.gif', 100, 40, 'shape=rounded');\n\t\t\t\taddVertex('editors/images/ellipse.gif', 40, 40, 'shape=ellipse');\n\t\t\t\taddVertex('editors/images/rhombus.gif', 40, 40, 'shape=rhombus');\n\t\t\t\taddVertex('editors/images/triangle.gif', 40, 40, 'shape=triangle');\n\t\t\t\taddVertex('editors/images/cylinder.gif', 40, 40, 'shape=cylinder');\n\t\t\t\taddVertex('editors/images/actor.gif', 30, 40, 'shape=actor');\n\t\t\t\ttoolbar.addLine();\n\t\t\t\t\n\t\t\t\tvar button = mxUtils.button('Create toolbar entry from selection', function(evt)\n\t\t\t\t{\n\t\t\t\t\tif (!graph.isSelectionEmpty())\n\t\t\t\t\t{\n\t\t\t\t\t\t// Creates a copy of the selection array to preserve its state\n\t\t\t\t\t\tvar cells = graph.getSelectionCells();\n\t\t\t\t\t\tvar bounds = graph.getView().getBounds(cells);\n\t\t\t\t\t\t\n\t\t\t\t\t\t// Function that is executed when the image is dropped on\n\t\t\t\t\t\t// the graph. The cell argument points to the cell under\n\t\t\t\t\t\t// the mousepointer if there is one.\n\t\t\t\t\t\tvar funct = function(graph, evt, cell)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tgraph.stopEditing(false);\n\t\t\t\n\t\t\t\t\t\t\tvar pt = graph.getPointForEvent(evt);\n\t\t\t\t\t\t\tvar dx = pt.x - bounds.x;\n\t\t\t\t\t\t\tvar dy = pt.y - bounds.y;\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tgraph.setSelectionCells(graph.importCells(cells, dx, dy, cell));\n\t\t\t\t\t\t}\n\t\t\t\n\t\t\t\t\t\t// Creates the image which is used as the drag icon (preview)\n\t\t\t\t\t\tvar img = toolbar.addMode(null, 'editors/images/outline.gif', funct);\n\t\t\t\t\t\tmxUtils.makeDraggable(img, graph, funct);\n\t\t\t\t\t}\n\t\t\t\t});\n\n\t\t\t\tbutton.style.position = 'absolute';\n\t\t\t\tbutton.style.left = '2px';\n\t\t\t\tbutton.style.top = '2px';\n\t\t\t\t\n\t\t\t\tdocument.body.appendChild(button);\n\t\t\t}\n\t\t}\n\n\t\tfunction addToolbarItem(graph, toolbar, prototype, image)\n\t\t{\n\t\t\t// Function that is executed when the image is dropped on\n\t\t\t// the graph. The cell argument points to the cell under\n\t\t\t// the mousepointer if there is one.\n\t\t\tvar funct = function(graph, evt, cell)\n\t\t\t{\n\t\t\t\tgraph.stopEditing(false);\n\n\t\t\t\tvar pt = graph.getPointForEvent(evt);\n\t\t\t\tvar vertex = graph.getModel().cloneCell(prototype);\n\t\t\t\tvertex.geometry.x = pt.x;\n\t\t\t\tvertex.geometry.y = pt.y;\n\t\t\t\t\n\t\t\t\tgraph.setSelectionCells(graph.importCells([vertex], 0, 0, cell));\n\t\t\t}\n\n\t\t\t// Creates the image which is used as the drag icon (preview)\n\t\t\tvar img = toolbar.addMode(null, image, funct);\n\t\t\tmxUtils.makeDraggable(img, graph, funct);\n\t\t}\n\n\t</script>\n</head>\n\n<!-- Calls the main function after the page has loaded. Container is dynamically created. -->\n<body>\nMelaneeeeeeeee!!!\n<br>\nMelaneeeeeeeee!!!\n<br>\nMelaneeeeeeeee!!!\n<br>\nMelaneeeeeeeee!!!\n<br>\n<script>main();</script>\n</body>\n</html>\n",
			"settings":
			{
				"buffer_size": 6085,
				"line_ending": "Unix"
			}
		}
	],
	"build_system": "",
	"build_system_choices":
	[
	],
	"build_varint": "",
	"command_palette":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"console":
	{
		"height": 0.0,
		"history":
		[
		]
	},
	"distraction_free":
	{
		"menu_visible": true,
		"show_minimap": false,
		"show_open_files": false,
		"show_tabs": false,
		"side_bar_visible": false,
		"status_bar_visible": false
	},
	"file_history":
	[
		"/Users/Wolvesville/Desktop/code",
		"/Users/Wolvesville/Downloads/packages-models-22-01/package.json",
		"/Users/Wolvesville/Downloads/melanee_web_v1-master/Melanee_web_v1/WebContent/toolbar3.jsp",
		"/Users/Wolvesville/Desktop/index2.html",
		"/Users/Wolvesville/Downloads/mxgraph-master/javascript/src/js/view/mxGraphSelectionModel.js"
	],
	"find":
	{
		"height": 39.0
	},
	"find_in_files":
	{
		"height": 0.0,
		"where_history":
		[
		]
	},
	"find_state":
	{
		"case_sensitive": false,
		"find_history":
		[
		],
		"highlight": true,
		"in_selection": false,
		"preserve_case": false,
		"regex": false,
		"replace_history":
		[
		],
		"reverse": false,
		"show_context": true,
		"use_buffer2": true,
		"whole_word": false,
		"wrap": true
	},
	"groups":
	[
		{
			"selected": 10,
			"sheets":
			[
				{
					"buffer": 0,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 20206,
						"regions":
						{
						},
						"selection":
						[
							[
								17783,
								17783
							]
						],
						"settings":
						{
							"auto_name": "% This is samplepaper.tex, a sample chapter demons",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 3506.0,
						"zoom_level": 1.0
					},
					"stack_index": 10,
					"type": "text"
				},
				{
					"buffer": 1,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 22376,
						"regions":
						{
						},
						"selection":
						[
							[
								21126,
								21130
							]
						],
						"settings":
						{
							"auto_name": "% This is samplepaper.tex, a sample chapter demons",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 2635.0,
						"zoom_level": 1.0
					},
					"stack_index": 8,
					"type": "text"
				},
				{
					"buffer": 2,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 25060,
						"regions":
						{
						},
						"selection":
						[
							[
								24699,
								24699
							]
						],
						"settings":
						{
							"auto_name": "% This is samplepaper.tex, a sample chapter demons",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 6020.0,
						"zoom_level": 1.0
					},
					"stack_index": 9,
					"type": "text"
				},
				{
					"buffer": 3,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 28021,
						"regions":
						{
						},
						"selection":
						[
							[
								28021,
								28021
							]
						],
						"settings":
						{
							"auto_name": "% This is samplepaper.tex, a sample chapter demons",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 6230.0,
						"zoom_level": 1.0
					},
					"stack_index": 7,
					"type": "text"
				},
				{
					"buffer": 4,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 29964,
						"regions":
						{
						},
						"selection":
						[
							[
								29964,
								29964
							]
						],
						"settings":
						{
							"auto_name": "% This is samplepaper.tex, a sample chapter demons",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 6545.0,
						"zoom_level": 1.0
					},
					"stack_index": 6,
					"type": "text"
				},
				{
					"buffer": 5,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 1455,
						"regions":
						{
						},
						"selection":
						[
							[
								1,
								1455
							]
						],
						"settings":
						{
							"auto_name": "",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 215.0,
						"zoom_level": 1.0
					},
					"stack_index": 4,
					"type": "text"
				},
				{
					"buffer": 6,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 29917,
						"regions":
						{
						},
						"selection":
						[
							[
								29917,
								29917
							]
						],
						"settings":
						{
							"auto_name": "% This is samplepaper.tex, a sample chapter demons",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 6305.0,
						"zoom_level": 1.0
					},
					"stack_index": 5,
					"type": "text"
				},
				{
					"buffer": 7,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 33815,
						"regions":
						{
						},
						"selection":
						[
							[
								33815,
								33815
							]
						],
						"settings":
						{
							"auto_name": "% This is samplepaper.tex, a sample chapter demons",
							"syntax": "Packages/Text/Plain text.tmLanguage"
						},
						"translation.x": 0.0,
						"translation.y": 7970.0,
						"zoom_level": 1.0
					},
					"stack_index": 3,
					"type": "text"
				},
				{
					"buffer": 8,
					"file": "/Users/Wolvesville/Downloads/melanee_web_project--master/src/main/webapp/WEB-INF/views/tool.jsp",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 90610,
						"regions":
						{
						},
						"selection":
						[
							[
								1151,
								1151
							]
						],
						"settings":
						{
							"syntax": "Packages/Java/Java Server Pages (JSP).sublime-syntax"
						},
						"translation.x": 0.0,
						"translation.y": 298.0,
						"zoom_level": 1.0
					},
					"stack_index": 2,
					"type": "text"
				},
				{
					"buffer": 9,
					"file": "toolbar.jsx",
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 5928,
						"regions":
						{
						},
						"selection":
						[
							[
								626,
								626
							]
						],
						"settings":
						{
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"tab_size": 2,
							"translate_tabs_to_spaces": true
						},
						"translation.x": 0.0,
						"translation.y": 365.0,
						"zoom_level": 1.0
					},
					"stack_index": 1,
					"type": "text"
				},
				{
					"buffer": 10,
					"semi_transient": false,
					"settings":
					{
						"buffer_size": 6085,
						"regions":
						{
						},
						"selection":
						[
							[
								6085,
								6085
							]
						],
						"settings":
						{
							"auto_name": "",
							"syntax": "Packages/Text/Plain text.tmLanguage",
							"translate_tabs_to_spaces": false
						},
						"translation.x": 0.0,
						"translation.y": 0.0,
						"zoom_level": 1.0
					},
					"stack_index": 0,
					"type": "text"
				}
			]
		}
	],
	"incremental_find":
	{
		"height": 24.0
	},
	"input":
	{
		"height": 0.0
	},
	"layout":
	{
		"cells":
		[
			[
				0,
				0,
				1,
				1
			]
		],
		"cols":
		[
			0.0,
			1.0
		],
		"rows":
		[
			0.0,
			1.0
		]
	},
	"menu_visible": true,
	"output.find_results":
	{
		"height": 0.0
	},
	"pinned_build_system": "",
	"project": "test.js",
	"replace":
	{
		"height": 44.0
	},
	"save_all_on_build": true,
	"select_file":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_project":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"select_symbol":
	{
		"height": 0.0,
		"last_filter": "",
		"selected_items":
		[
		],
		"width": 0.0
	},
	"selected_group": 0,
	"settings":
	{
	},
	"show_minimap": true,
	"show_open_files": false,
	"show_tabs": true,
	"side_bar_visible": true,
	"side_bar_width": 150.0,
	"status_bar_visible": true,
	"template_settings":
	{
	}
}
